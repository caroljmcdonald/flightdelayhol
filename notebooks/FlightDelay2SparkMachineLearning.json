{"paragraphs":[{"text":"%md\n# Fast data processing pipeline for predicting flight delays using Apache APIs: Kafka, Spark Streaming and Machine Learning\nNote: Make sure you followed the instructions in Step one to copy the data file to MapR XD\n## Part 2: Spark Machine Learning\nThis notebook is part two of a series of notebooks demonstrating an end-to-end application that combines streaming data with machine learning and fast storage.\n* Part 1 of this series shows how to explore the Flight Dataset with Spark SQL \n* Part 2 shows how to use Apache Spark’s ML pipelines with a Random Forest Classifier to predict flight delays\n* Part 3 shows how to use this model with Streaming Data. \nFor more information, see [this](https://mapr.com/ebook/getting-started-with-apache-spark-v2/)  MapR Spark ebook.\n<img src=\"https://mapr.com/blog/datasets-dataframes-and-spark-sql-for-processing-of-tabular-data/assets/image4.jpg\">","user":"anonymous","dateUpdated":"2019-03-15T22:39:09+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Fast data processing pipeline for predicting flight delays using Apache APIs: Kafka, Spark Streaming and Machine Learning</h1>\n<p>Note: Make sure you followed the instructions in Step one to copy the data file to MapR XD</p>\n<h2>Part 2: Spark Machine Learning</h2>\n<p>This notebook is part two of a series of notebooks demonstrating an end-to-end application that combines streaming data with machine learning and fast storage.<br/>* Part 1 of this series shows how to explore the Flight Dataset with Spark SQL<br/>* Part 2 shows how to use Apache Spark’s ML pipelines with a Random Forest Classifier to predict flight delays<br/>* Part 3 shows how to use this model with Streaming Data.<br/>For more information, see <a href=\"https://mapr.com/ebook/getting-started-with-apache-spark-v2/\">this</a> MapR Spark ebook.<br/><img src=\"https://mapr.com/blog/datasets-dataframes-and-spark-sql-for-processing-of-tabular-data/assets/image4.jpg\"></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1552688993743_-852321028","id":"20190220-173254_1728425170","dateCreated":"2019-03-15T22:29:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:6232","dateFinished":"2019-03-15T22:39:09+0000","dateStarted":"2019-03-15T22:39:09+0000"},{"text":"%md \n## Supervised Machine Learning Classification\nSupervised algorithms use labeled data in which both the input and target outcome, or label, are provided to the algorithm. Supervised learning is also called predictive modeling or predictive analytics, because you build a model that is capable of making predictions. \n<img src=\"https://mapr.com/blog/apache-spark-machine-learning-tutorial/assets/image13.png\" width=\"500\" height=\"500\">\nClassification takes a set of data with known labels and predetermined features and learns how to label new records based on that information. Features are the \"if questions\" that you ask. The label is the answer to those questions. Let’s go through an example for flight delays: \nWhat are we trying to predict?\n* Whether a flight will be delayed or not.\n* Delayed is the Label: True or False\n\nWhat are the “if questions” or properties that you can use to make predictions?\n* What is the Originating Airport?\n* What is the Destination Airport?\n* What is the Scheduled time of departure?\n* What is the scheduled time of arrival?\n* What is the day of the week?\n* What is the Airline Carrier?\n\n### Decision Trees \nDecision trees create a model that predicts the label (or class) by evaluating a set of rules that follow an IF THEN ELSE…pattern. The If then ELSE feature questions are the nodes, and the answers True or false are the branches in the tree to the child nodes. A decision tree model estimates the minimum number of true/false questions needed, to assess the probability of making a correct decision. Below is an example of a simplified decision tree for flight delays:\n<img src=\"https://mapr.com/blog/fast-data-processing-pipeline-predicting-flight-delays-using-apache-apis-pt-1/assets/decision-tree.png\" width=\"600\" height=\"600\">\n\n### Random Forests\nEnsemble learning algorithms combine multiple machine learning algorithms to obtain a better model. Random Forest is a popular ensemble learning method for Classification and regression. The algorithm builds a model consisting of multiple decision trees , based on different subsets of data at the training stage. Predictions are made by combining the output from all of the trees which reduces the variance, and improves the predictive accuracy. For Random Forest Classification each tree’s prediction is counted as a vote for one class. The label is predicted to be the class which receives the most votes.\n<img src=\"https://mapr.com/blog/predicting-loan-credit-risk-using-apache-spark-machine-learning-random-forests/assets/blogimages/sparkmlrandomforest.png\" >\n\n### Flight Data \nOur Flight Dataset comes from flight information from the United States Department of Transportation for the year 2018.\nThe flight data is in JSON files, with each flight having the following information:\n\nid: ID composed of origin_destination_date_carrier_flight number\ndofW: day of week (1=Monday, 7=Sunday)\ncarrier: carrier code\nsrc: originating airport code\ndst: destination airport code\ncrsdephour: scheduled departure hour\ncrsdeptime: scheduled departure time\ndepdelay: departure delay in minutes\ncrsarrtime: scheduled arrival time\narrdelay: arrival delay minutes\ncrselapsedtime: elapsed time\ndist: distance\n\nIt appears in the following format:\n{\n    \"id\": \"ATL_BOS_2018-01-01_DL_104\",\n    \"fldate\": \"2018-01-01\",\n    \"month\": 1,\n    \"dofW\": 1,\n    \"carrier\": \"DL\",\n    \"src\": \"ATL\",\n    \"dst\": \"BOS\",\n    \"crsdephour\": 9,\n    \"crsdeptime\": 850,\n    \"depdelay\": 0.0,\n    \"crsarrtime\": 1116,\n    \"arrdelay\": 0.0,\n    \"crselapsedtime\": 146.0,\n    \"dist\": 946.0\n}\n","user":"anonymous","dateUpdated":"2019-03-15T22:29:53+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Supervised Machine Learning Classification</h2>\n<p>Supervised algorithms use labeled data in which both the input and target outcome, or label, are provided to the algorithm. Supervised learning is also called predictive modeling or predictive analytics, because you build a model that is capable of making predictions.<br/><img src=\"https://mapr.com/blog/apache-spark-machine-learning-tutorial/assets/image13.png\" width=\"500\" height=\"500\"><br/>Classification takes a set of data with known labels and predetermined features and learns how to label new records based on that information. Features are the &ldquo;if questions&rdquo; that you ask. The label is the answer to those questions. Let’s go through an example for flight delays:<br/>What are we trying to predict?<br/>* Whether a flight will be delayed or not.<br/>* Delayed is the Label: True or False</p>\n<p>What are the “if questions” or properties that you can use to make predictions?<br/>* What is the Originating Airport?<br/>* What is the Destination Airport?<br/>* What is the Scheduled time of departure?<br/>* What is the scheduled time of arrival?<br/>* What is the day of the week?<br/>* What is the Airline Carrier?</p>\n<h3>Decision Trees</h3>\n<p>Decision trees create a model that predicts the label (or class) by evaluating a set of rules that follow an IF THEN ELSE…pattern. The If then ELSE feature questions are the nodes, and the answers True or false are the branches in the tree to the child nodes. A decision tree model estimates the minimum number of true/false questions needed, to assess the probability of making a correct decision. Below is an example of a simplified decision tree for flight delays:<br/><img src=\"https://mapr.com/blog/fast-data-processing-pipeline-predicting-flight-delays-using-apache-apis-pt-1/assets/decision-tree.png\" width=\"600\" height=\"600\"></p>\n<h3>Random Forests</h3>\n<p>Ensemble learning algorithms combine multiple machine learning algorithms to obtain a better model. Random Forest is a popular ensemble learning method for Classification and regression. The algorithm builds a model consisting of multiple decision trees , based on different subsets of data at the training stage. Predictions are made by combining the output from all of the trees which reduces the variance, and improves the predictive accuracy. For Random Forest Classification each tree’s prediction is counted as a vote for one class. The label is predicted to be the class which receives the most votes.<br/><img src=\"https://mapr.com/blog/predicting-loan-credit-risk-using-apache-spark-machine-learning-random-forests/assets/blogimages/sparkmlrandomforest.png\" ></p>\n<h3>Flight Data</h3>\n<p>Our Flight Dataset comes from flight information from the United States Department of Transportation for the year 2018.<br/>The flight data is in JSON files, with each flight having the following information:</p>\n<p>id: ID composed of origin_destination_date_carrier_flight number<br/>dofW: day of week (1=Monday, 7=Sunday)<br/>carrier: carrier code<br/>src: originating airport code<br/>dst: destination airport code<br/>crsdephour: scheduled departure hour<br/>crsdeptime: scheduled departure time<br/>depdelay: departure delay in minutes<br/>crsarrtime: scheduled arrival time<br/>arrdelay: arrival delay minutes<br/>crselapsedtime: elapsed time<br/>dist: distance</p>\n<p>It appears in the following format:<br/>{<br/> &ldquo;id&rdquo;: &ldquo;ATL_BOS_2018-01-01_DL_104&rdquo;,<br/> &ldquo;fldate&rdquo;: &ldquo;2018-01-01&rdquo;,<br/> &ldquo;month&rdquo;: 1,<br/> &ldquo;dofW&rdquo;: 1,<br/> &ldquo;carrier&rdquo;: &ldquo;DL&rdquo;,<br/> &ldquo;src&rdquo;: &ldquo;ATL&rdquo;,<br/> &ldquo;dst&rdquo;: &ldquo;BOS&rdquo;,<br/> &ldquo;crsdephour&rdquo;: 9,<br/> &ldquo;crsdeptime&rdquo;: 850,<br/> &ldquo;depdelay&rdquo;: 0.0,<br/> &ldquo;crsarrtime&rdquo;: 1116,<br/> &ldquo;arrdelay&rdquo;: 0.0,<br/> &ldquo;crselapsedtime&rdquo;: 146.0,<br/> &ldquo;dist&rdquo;: 946.0<br/>}</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1552688993750_-212155511","id":"20170530-122945_1594214131","dateCreated":"2019-03-15T22:29:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6233"},{"title":"Import needed packages","text":"%spark\nimport org.apache.spark._\nimport org.apache.spark.ml._\nimport org.apache.spark.ml.feature._\nimport org.apache.spark.ml.classification._\nimport org.apache.spark.ml.evaluation._\nimport org.apache.spark.ml.tuning._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._","user":"anonymous","dateUpdated":"2019-03-15T22:29:53+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark._\nimport org.apache.spark.ml._\nimport org.apache.spark.ml.feature._\nimport org.apache.spark.ml.classification._\nimport org.apache.spark.ml.evaluation._\nimport org.apache.spark.ml.tuning._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._\n"}]},"apps":[],"jobName":"paragraph_1552688993751_1380450997","id":"20170508-144514_403247535","dateCreated":"2019-03-15T22:29:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6234"},{"title":"Define Schema for JSON file data","text":"%spark\n// We use a Spark SQL Structype to define the Dataset schema, \n// corresponding to a line in the JSON data file.\n\n  val schema = StructType(Array(\n    StructField(\"id\", StringType, true),\n    StructField(\"fldate\", StringType, true),\n    StructField(\"month\", IntegerType, true),\n    StructField(\"dofW\", IntegerType, true),\n    StructField(\"carrier\", StringType, true),\n    StructField(\"src\", StringType, true),\n    StructField(\"dst\", StringType, true),\n    StructField(\"crsdephour\", IntegerType, true),\n    StructField(\"crsdeptime\", IntegerType, true),\n    StructField(\"depdelay\", DoubleType, true),\n    StructField(\"crsarrtime\", IntegerType, true),\n    StructField(\"arrdelay\", DoubleType, true),\n    StructField(\"crselapsedtime\", DoubleType, true),\n    StructField(\"dist\", DoubleType, true)\n  ))\n   ","user":"anonymous","dateUpdated":"2019-03-15T22:29:53+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"schema: org.apache.spark.sql.types.StructType = StructType(StructField(id,StringType,true), StructField(fldate,StringType,true), StructField(month,IntegerType,true), StructField(dofW,IntegerType,true), StructField(carrier,StringType,true), StructField(src,StringType,true), StructField(dst,StringType,true), StructField(crsdephour,IntegerType,true), StructField(crsdeptime,IntegerType,true), StructField(depdelay,DoubleType,true), StructField(crsarrtime,IntegerType,true), StructField(arrdelay,DoubleType,true), StructField(crselapsedtime,DoubleType,true), StructField(dist,DoubleType,true))\n"}]},"apps":[],"jobName":"paragraph_1552688993751_1836154046","id":"20170508-150032_326029627","dateCreated":"2019-03-15T22:29:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6235"},{"title":"Read the data from JSON file into a Dataset of type Flight","text":"import spark.implicits._\n\n// file path location\n var file =\"/user/mapr/data/flightdata2018.json\"\n// With the SparkSession read method, read data from a file into a Dataset\n val df = spark.read.format(\"json\").option(\"inferSchema\", \"false\").schema(schema).load(file)\n// display the first 20 rows of the DataFrame\n df.show\n","user":"anonymous","dateUpdated":"2019-03-15T22:29:53+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+----------+-----+----+-------+---+---+----------+----------+--------+----------+--------+--------------+-----+\n|                  id|    fldate|month|dofW|carrier|src|dst|crsdephour|crsdeptime|depdelay|crsarrtime|arrdelay|crselapsedtime| dist|\n+--------------------+----------+-----+----+-------+---+---+----------+----------+--------+----------+--------+--------------+-----+\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|BOS|         9|       850|     0.0|      1116|     0.0|         146.0|946.0|\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|BOS|        11|      1122|     8.0|      1349|     0.0|         147.0|946.0|\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|BOS|        14|      1356|     9.0|      1623|     0.0|         147.0|946.0|\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|BOS|        16|      1620|     0.0|      1851|     3.0|         151.0|946.0|\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|BOS|        19|      1940|     6.0|      2210|     0.0|         150.0|946.0|\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|BOS|        12|      1248|     0.0|      1513|     0.0|         145.0|946.0|\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|BOS|        22|      2215|     0.0|        39|     0.0|         144.0|946.0|\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|BOS|        15|      1500|    21.0|      1734|    33.0|         154.0|946.0|\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     WN|ATL|BOS|        15|      1500|   198.0|      1725|   208.0|         145.0|946.0|\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     WN|ATL|BOS|        21|      2055|    14.0|      2330|     0.0|         155.0|946.0|\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     WN|ATL|BOS|        10|      1015|   215.0|      1250|   191.0|         155.0|946.0|\n|ATL_CLT_2018-01-0...|2018-01-01|    1|   1|     AA|ATL|CLT|        11|      1114|     0.0|      1238|     0.0|          84.0|226.0|\n|ATL_CLT_2018-01-0...|2018-01-01|    1|   1|     AA|ATL|CLT|         8|       845|     0.0|      1011|     0.0|          86.0|226.0|\n|ATL_CLT_2018-01-0...|2018-01-01|    1|   1|     AA|ATL|CLT|        15|      1548|     0.0|      1710|     0.0|          82.0|226.0|\n|ATL_CLT_2018-01-0...|2018-01-01|    1|   1|     AA|ATL|CLT|         7|       705|     0.0|       821|     0.0|          76.0|226.0|\n|ATL_CLT_2018-01-0...|2018-01-01|    1|   1|     AA|ATL|CLT|        12|      1226|     0.0|      1347|     0.0|          81.0|226.0|\n|ATL_CLT_2018-01-0...|2018-01-01|    1|   1|     AA|ATL|CLT|        22|      2205|     0.0|      2319|     1.0|          74.0|226.0|\n|ATL_CLT_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|CLT|        22|      2210|    11.0|      2324|     0.0|          74.0|226.0|\n|ATL_CLT_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|CLT|        15|      1543|     1.0|      1659|     0.0|          76.0|226.0|\n|ATL_CLT_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|CLT|        10|      1008|     0.0|      1124|     0.0|          76.0|226.0|\n+--------------------+----------+-----+----+-------+---+---+----------+----------+--------+----------+--------+--------------+-----+\nonly showing top 20 rows\n\nimport spark.implicits._\nfile: String = /user/mapr/data/flightdata2018.json\ndf: org.apache.spark.sql.DataFrame = [id: string, fldate: string ... 12 more fields]\n"}]},"apps":[],"jobName":"paragraph_1552688993752_-1048205070","id":"20170508-150131_378637203","dateCreated":"2019-03-15T22:29:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6236"},{"text":"%md\nThe describe() function performs summary statistics calculations on  numeric columns ","user":"anonymous","dateUpdated":"2019-03-15T22:29:53+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>The describe() function performs summary statistics calculations on numeric columns</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1552688993752_-1808812051","id":"20170524-214640_973339640","dateCreated":"2019-03-15T22:29:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6237"},{"title":"Add origin_dest column  and display the first 20 rows","text":"// Here we add a column to the DataFrame named orig_dest, consisting of the originating and destination airports\nval df1 = df.withColumn(\"orig_dest\", concat($\"src\",lit(\"_\"), $\"dst\"))\ndf1.show()\n\n","user":"anonymous","dateUpdated":"2019-03-15T22:29:53+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+----------+-----+----+-------+---+---+----------+----------+--------+----------+--------+--------------+-----+---------+\n|                  id|    fldate|month|dofW|carrier|src|dst|crsdephour|crsdeptime|depdelay|crsarrtime|arrdelay|crselapsedtime| dist|orig_dest|\n+--------------------+----------+-----+----+-------+---+---+----------+----------+--------+----------+--------+--------------+-----+---------+\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|BOS|         9|       850|     0.0|      1116|     0.0|         146.0|946.0|  ATL_BOS|\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|BOS|        11|      1122|     8.0|      1349|     0.0|         147.0|946.0|  ATL_BOS|\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|BOS|        14|      1356|     9.0|      1623|     0.0|         147.0|946.0|  ATL_BOS|\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|BOS|        16|      1620|     0.0|      1851|     3.0|         151.0|946.0|  ATL_BOS|\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|BOS|        19|      1940|     6.0|      2210|     0.0|         150.0|946.0|  ATL_BOS|\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|BOS|        12|      1248|     0.0|      1513|     0.0|         145.0|946.0|  ATL_BOS|\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|BOS|        22|      2215|     0.0|        39|     0.0|         144.0|946.0|  ATL_BOS|\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|BOS|        15|      1500|    21.0|      1734|    33.0|         154.0|946.0|  ATL_BOS|\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     WN|ATL|BOS|        15|      1500|   198.0|      1725|   208.0|         145.0|946.0|  ATL_BOS|\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     WN|ATL|BOS|        21|      2055|    14.0|      2330|     0.0|         155.0|946.0|  ATL_BOS|\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     WN|ATL|BOS|        10|      1015|   215.0|      1250|   191.0|         155.0|946.0|  ATL_BOS|\n|ATL_CLT_2018-01-0...|2018-01-01|    1|   1|     AA|ATL|CLT|        11|      1114|     0.0|      1238|     0.0|          84.0|226.0|  ATL_CLT|\n|ATL_CLT_2018-01-0...|2018-01-01|    1|   1|     AA|ATL|CLT|         8|       845|     0.0|      1011|     0.0|          86.0|226.0|  ATL_CLT|\n|ATL_CLT_2018-01-0...|2018-01-01|    1|   1|     AA|ATL|CLT|        15|      1548|     0.0|      1710|     0.0|          82.0|226.0|  ATL_CLT|\n|ATL_CLT_2018-01-0...|2018-01-01|    1|   1|     AA|ATL|CLT|         7|       705|     0.0|       821|     0.0|          76.0|226.0|  ATL_CLT|\n|ATL_CLT_2018-01-0...|2018-01-01|    1|   1|     AA|ATL|CLT|        12|      1226|     0.0|      1347|     0.0|          81.0|226.0|  ATL_CLT|\n|ATL_CLT_2018-01-0...|2018-01-01|    1|   1|     AA|ATL|CLT|        22|      2205|     0.0|      2319|     1.0|          74.0|226.0|  ATL_CLT|\n|ATL_CLT_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|CLT|        22|      2210|    11.0|      2324|     0.0|          74.0|226.0|  ATL_CLT|\n|ATL_CLT_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|CLT|        15|      1543|     1.0|      1659|     0.0|          76.0|226.0|  ATL_CLT|\n|ATL_CLT_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|CLT|        10|      1008|     0.0|      1124|     0.0|          76.0|226.0|  ATL_CLT|\n+--------------------+----------+-----+----+-------+---+---+----------+----------+--------+----------+--------+--------------+-----+---------+\nonly showing top 20 rows\n\ndf1: org.apache.spark.sql.DataFrame = [id: string, fldate: string ... 13 more fields]\n"}]},"apps":[],"jobName":"paragraph_1552688993753_-523823952","id":"20171129-221736_959969733","dateCreated":"2019-03-15T22:29:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6238"},{"text":"%md\nIn the code below a Spark Bucketizer is used to split the dataset into delayed and not delayed flights (where delayed > 40 minutes ) \nwith a delayed 0/1 column. Then the resulting total counts are displayed. ","user":"anonymous","dateUpdated":"2019-03-15T22:29:53+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>In the code below a Spark Bucketizer is used to split the dataset into delayed and not delayed flights (where delayed &gt; 40 minutes )<br/>with a delayed 0/1 column. Then the resulting total counts are displayed.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1552688993754_-687353244","id":"20190305-221927_113621865","dateCreated":"2019-03-15T22:29:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6239"},{"text":"%md\n## Stratified Sampling\nIn order to ensure that our model is sensitive to the delayed samples we can put the two sample types on the same footing using stratified sampling. \nFirst a Spark Bucketizer is used to split the dataset into delayed and not delayed flights (where delayed > 40 minutes ) with a delayed 0/1 column.\nThen the DataFrames sampleBy() function is used to stratify, by providing the fractions of each sample type to be returned.\nHere, we're keeping all instances of delayed, but downsampling the not delayed instances to 13%, then displaying the results\n","user":"anonymous","dateUpdated":"2019-03-15T22:29:53+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Stratified Sampling</h2>\n<p>In order to ensure that our model is sensitive to the delayed samples we can put the two sample types on the same footing using stratified sampling.<br/>First a Spark Bucketizer is used to split the dataset into delayed and not delayed flights (where delayed &gt; 40 minutes ) with a delayed 0/1 column.<br/>Then the DataFrames sampleBy() function is used to stratify, by providing the fractions of each sample type to be returned.<br/>Here, we&rsquo;re keeping all instances of delayed, but downsampling the not delayed instances to 13%, then displaying the results</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1552688993754_1088364271","id":"20190305-223154_803200822","dateCreated":"2019-03-15T22:29:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6240"},{"title":"Stratify the sampling to fewer Not Delayed","text":"// bucket > 40 minutes = delayed\nval delaybucketizer = new Bucketizer().setInputCol(\"depdelay\").setOutputCol(\"delayed\").setSplits(Array(0.0,41.0,Double.PositiveInfinity))\nval df2= delaybucketizer.transform(df1)\nprintln(\"original distribution\")\ndf2.groupBy(\"delayed\").count.show\n// keep all delayed , keep 13% not delayed\nval fractions = Map(0.0 -> .13, 1.0->1.0) // \nval df3 =df2.stat.sampleBy(\"delayed\", fractions, 36L)\nprintln(\"current distribution\")\ndf3.groupBy(\"delayed\").count.show\n","user":"anonymous","dateUpdated":"2019-03-15T22:29:53+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"original distribution\n+-------+------+\n|delayed| count|\n+-------+------+\n|    0.0|253309|\n|    1.0| 29319|\n+-------+------+\n\ncurrent distribution\n+-------+-----+\n|delayed|count|\n+-------+-----+\n|    0.0|33063|\n|    1.0|29319|\n+-------+-----+\n\ndelaybucketizer: org.apache.spark.ml.feature.Bucketizer = bucketizer_37dd44a1f10d\ndf2: org.apache.spark.sql.DataFrame = [id: string, fldate: string ... 14 more fields]\nfractions: scala.collection.immutable.Map[Double,Double] = Map(0.0 -> 0.13, 1.0 -> 1.0)\ndf3: org.apache.spark.sql.DataFrame = [id: string, fldate: string ... 14 more fields]\n"}]},"apps":[],"jobName":"paragraph_1552688993755_1159801709","id":"20171123-050717_1328190278","dateCreated":"2019-03-15T22:29:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6241"},{"text":"%md\nThere are typically two phases in machine learning:\n* Data Discovery: The first phase involves analysis on historical data to build and train the machine learning model.\n* Analytics Using the Model: The second phase uses the model in production on new data.\n\nIn production, models need to be continuously monitored and updated with new models when needed.\n<img src=\"https://mapr.com/blog/apache-spark-machine-learning-tutorial/assets/image21.png\">\nIn this lab the process has been simplified due to time and memory constraints(one node per student). We will split out dataset into a training and test set and evaluate the results on the test set. In the real world more data and iterations of training and evaluation with k-fold cross validation is recommended. ","user":"anonymous","dateUpdated":"2019-03-15T22:29:53+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>There are typically two phases in machine learning:<br/>* Data Discovery: The first phase involves analysis on historical data to build and train the machine learning model.<br/>* Analytics Using the Model: The second phase uses the model in production on new data.</p>\n<p>In production, models need to be continuously monitored and updated with new models when needed.<br/><img src=\"https://mapr.com/blog/apache-spark-machine-learning-tutorial/assets/image21.png\"><br/>In this lab the process has been simplified due to time and memory constraints(one node per student). We will split out dataset into a training and test set and evaluate the results on the test set. In the real world more data and iterations of training and evaluation with k-fold cross validation is recommended.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1552688993756_-1188470980","id":"20190308-210308_1297732685","dateCreated":"2019-03-15T22:29:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6242"},{"title":"Split Into Training and Test set","text":"val splitSeed = 5043\nval Array(trainingData, testData) = df3.randomSplit(Array(0.7, 0.3), splitSeed)\n\ntrainingData.cache\ntrainingData.groupBy(\"delayed\").count.show","user":"anonymous","dateUpdated":"2019-03-15T22:29:53+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+-----+\n|delayed|count|\n+-------+-----+\n|    0.0|23149|\n|    1.0|20500|\n+-------+-----+\n\nsplitSeed: Int = 5043\ntrainingData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: string, fldate: string ... 14 more fields]\ntestData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: string, fldate: string ... 14 more fields]\n"}]},"apps":[],"jobName":"paragraph_1552688993756_-1332042448","id":"20190305-223741_1028691135","dateCreated":"2019-03-15T22:29:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6243"},{"text":"%md\n## Feature Extraction and Pipelining\nIn order for the features to be used by a machine learning algorithm, they must be transformed and put into Feature Vectors, which are vectors of numbers \nrepresenting the value for each feature.\n<img src=\"https://mapr.com/blog/fast-data-processing-pipeline-predicting-flight-delays-using-apache-apis-pt-1/assets/reference-learning-spark.png\" width=\"600\" height=\"600\">\nSpark ML provides a uniform set of high-level APIs built on top of DataFrames. We will use an ML Pipeline to pass the data through transformers in order to extract the features and an estimator to produce the model.\n<img src=\"https://mapr.com/blog/apache-spark-machine-learning-tutorial/assets/image8.png\" width=\"600\" height=\"600\">\nTransformer: A Transformer is an algorithm which transforms one DataFrame into another DataFrame. We will use a transformer to get a DataFrame with a features vector column.\nEstimator: An Estimator is an algorithm which can be fit on a DataFrame to produce a Transformer. We will use a a Random Forest estimator to train a model which can transform data to get predictions.\nPipeline: A Pipeline chains multiple Transformers and Estimators together to specify a ML workflow. \nWe will use a Spark ML Pipeline to pass our data through transformers in order to extract the features, an estimator to produce a model.\nThe we will use an evaluator to evaluate the model on the test data. \n\n\n","user":"anonymous","dateUpdated":"2019-03-15T22:29:53+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Feature Extraction and Pipelining</h2>\n<p>In order for the features to be used by a machine learning algorithm, they must be transformed and put into Feature Vectors, which are vectors of numbers<br/>representing the value for each feature.<br/><img src=\"https://mapr.com/blog/fast-data-processing-pipeline-predicting-flight-delays-using-apache-apis-pt-1/assets/reference-learning-spark.png\" width=\"600\" height=\"600\"><br/>Spark ML provides a uniform set of high-level APIs built on top of DataFrames. We will use an ML Pipeline to pass the data through transformers in order to extract the features and an estimator to produce the model.<br/><img src=\"https://mapr.com/blog/apache-spark-machine-learning-tutorial/assets/image8.png\" width=\"600\" height=\"600\"><br/>Transformer: A Transformer is an algorithm which transforms one DataFrame into another DataFrame. We will use a transformer to get a DataFrame with a features vector column.<br/>Estimator: An Estimator is an algorithm which can be fit on a DataFrame to produce a Transformer. We will use a a Random Forest estimator to train a model which can transform data to get predictions.<br/>Pipeline: A Pipeline chains multiple Transformers and Estimators together to specify a ML workflow.<br/>We will use a Spark ML Pipeline to pass our data through transformers in order to extract the features, an estimator to produce a model.<br/>The we will use an evaluator to evaluate the model on the test data.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1552688993757_-443125244","id":"20190305-223449_516878384","dateCreated":"2019-03-15T22:29:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6244"},{"text":"%md\nThe ML package needs the label and feature vector to be added as columns to a dataframe. \nBelow we set up a pipeline to pass the data through transformers in order to extract the features and label. \nA StringIndexer is used to to encode a string column to a column of number indices.\nA Bucketizer is used to add a label of delayed 0/1. \nA VectorAssembler combines a given list of columns into a single feature vector column.\n<img src=\"https://mapr.com/blog/apache-spark-machine-learning-tutorial/assets/image19.png\" width=\"600\" height=\"600\">\n","user":"anonymous","dateUpdated":"2019-03-15T22:29:53+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>The ML package needs the label and feature vector to be added as columns to a dataframe.<br/>Below we set up a pipeline to pass the data through transformers in order to extract the features and label.<br/>A StringIndexer is used to to encode a string column to a column of number indices.<br/>A Bucketizer is used to add a label of delayed 0/1.<br/>A VectorAssembler combines a given list of columns into a single feature vector column.<br/><img src=\"https://mapr.com/blog/apache-spark-machine-learning-tutorial/assets/image19.png\" width=\"600\" height=\"600\"></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1552688993757_-2100225302","id":"20170603-184811_78732818","dateCreated":"2019-03-15T22:29:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6245"},{"title":"Use a StringIndexer  to encode categorical columns","text":"// categorical Column names\nval categoricalColumns = Array(\"carrier\", \"src\", \"dst\", \"dofW\", \"orig_dest\")\n\n// a StringIndexer will encode a string categorial column into a column of numbers\nval stringIndexers = categoricalColumns.map { colName =>\n      new StringIndexer()\n        .setInputCol(colName)\n        .setOutputCol(colName + \"Indexed\")\n        .fit(trainingData)\n}\n","user":"anonymous","dateUpdated":"2019-03-15T22:29:53+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"categoricalColumns: Array[String] = Array(carrier, src, dst, dofW, orig_dest)\nstringIndexers: Array[org.apache.spark.ml.feature.StringIndexerModel] = Array(strIdx_3826d51ed04e, strIdx_c703d0663326, strIdx_ab30cef1bd66, strIdx_0a0de03085b7, strIdx_181d229dc5bc)\n"}]},"apps":[],"jobName":"paragraph_1552688993758_-1467546918","id":"20170508-150543_958647761","dateCreated":"2019-03-15T22:29:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6246"},{"title":"Use VectorAssembler, a transformer,  to put features into a feature vector column","text":"//a Bucketizer is used to add a label column of delayed 0/1.\nval labeler = new Bucketizer().setInputCol(\"depdelay\").setOutputCol(\"label\").setSplits(Array( 0.0, 40.0, Double.PositiveInfinity))\n\n// list of all the feature columns\nval featureCols = Array(\"carrierIndexed\", \"dstIndexed\", \"srcIndexed\", \"dofWIndexed\", \"orig_destIndexed\",\"crsdephour\", \"crsdeptime\", \"crsarrtime\",\"crselapsedtime\", \"dist\")\n\n//The VectorAssembler combines a given list of columns into a single feature vector column. \nval assembler = new VectorAssembler().setInputCols(featureCols).setOutputCol(\"features\")\n\n","user":"anonymous","dateUpdated":"2019-03-15T22:29:53+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"labeler: org.apache.spark.ml.feature.Bucketizer = bucketizer_5d6be9021b93\nfeatureCols: Array[String] = Array(carrierIndexed, dstIndexed, srcIndexed, dofWIndexed, orig_destIndexed, crsdephour, crsdeptime, crsarrtime, crselapsedtime, dist)\nassembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_1f550c68f1a4\n"}]},"apps":[],"jobName":"paragraph_1552688993758_-437710034","id":"20170524-223310_2121058884","dateCreated":"2019-03-15T22:29:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6247"},{"title":"Create Random Forest Estimator , set Label and Feature Columns ","text":"// The final element in our ml pipeline is an estimator (a random forest classifier), \n// which will training on the vector of label and features.\nval rf = new RandomForestClassifier().setLabelCol(\"label\").setFeaturesCol(\"features\").setNumTrees(10).setMaxBins(1000).setMaxDepth(8)    ","user":"anonymous","dateUpdated":"2019-03-15T22:29:53+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"rf: org.apache.spark.ml.classification.RandomForestClassifier = rfc_665455f14c70\n"}]},"apps":[],"jobName":"paragraph_1552688993759_1277565816","id":"20170603-185445_276463997","dateCreated":"2019-03-15T22:29:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6248"},{"text":"%md\n## Setup Spark ML pipeline stages\nSet up a pipeline to pass the data through transformers to extract the features and label, and pass this to a random forest estimator to fit the model \n","user":"anonymous","dateUpdated":"2019-03-15T22:29:53+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Setup Spark ML pipeline stages</h2>\n<p>Set up a pipeline to pass the data through transformers to extract the features and label, and pass this to a random forest estimator to fit the model</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1552688993760_175633239","id":"20170601-154525_1033166149","dateCreated":"2019-03-15T22:29:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6249"},{"title":"Set up pipeline with  feature transformers and model estimator","text":"// Below we chain the stringindexers, vector assembler and randomforest in a Pipeline.\nval steps = stringIndexers ++ Array(labeler, assembler, rf)\nval pipeline = new Pipeline().setStages(steps)\n","user":"anonymous","dateUpdated":"2019-03-15T22:29:53+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"steps: Array[org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.MLWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.MLWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.MLWritable}}] = Array(strIdx_3826d51ed04e, strIdx_c703d0663326, strIdx_ab30cef1bd66, strIdx_0a0de03085b7, strIdx_181d229dc5bc, bucketizer_5d6be9021b93, vecAssembler_1f550c68f1a4, rfc_665455f14c70)\npipeline: org.apache.spark.ml.Pipeline = pipeline_5da9e722de12\n"}]},"apps":[],"jobName":"paragraph_1552688993760_2023112646","id":"20170508-151557_1422077156","dateCreated":"2019-03-15T22:29:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6250"},{"text":"%md\n### Train the Model\nWe would like to determine which parameter values of the decision tree produce the best model. A common technique for model selection is k-fold cross validation, \nwhere the data is randomly split into partitions. Each partition is used once as the testing data set, while the rest are used for training.\nModels are then generated using the training sets and evaluated with the testing sets, resulting in k model performance measurements. \nThe model parameters leading to the highest performance metric produce the best model. \n<img src=\"https://mapr.com/blog/fast-data-processing-pipeline-predicting-flight-delays-using-apache-apis-pt-1/assets/ml-cross-validation-process.png\" width=\"600\" height=\"600\">\nDue to memory and time constraints for this lab we will not perfrom k-fold cross validation in this lab, but you can look at this blog for more information https://mapr.com/blog/fast-data-processing-pipeline-predicting-flight-delays-using-apache-apis-pt-1/\nIn this lab we will perform one iteration to train on the training data set and test on the test data set.\n","user":"anonymous","dateUpdated":"2019-03-15T22:29:53+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Train the Model</h3>\n<p>We would like to determine which parameter values of the decision tree produce the best model. A common technique for model selection is k-fold cross validation,<br/>where the data is randomly split into partitions. Each partition is used once as the testing data set, while the rest are used for training.<br/>Models are then generated using the training sets and evaluated with the testing sets, resulting in k model performance measurements.<br/>The model parameters leading to the highest performance metric produce the best model.<br/><img src=\"https://mapr.com/blog/fast-data-processing-pipeline-predicting-flight-delays-using-apache-apis-pt-1/assets/ml-cross-validation-process.png\" width=\"600\" height=\"600\"><br/>Due to memory and time constraints for this lab we will not perfrom k-fold cross validation in this lab, but you can look at this blog for more information <a href=\"https://mapr.com/blog/fast-data-processing-pipeline-predicting-flight-delays-using-apache-apis-pt-1/\">https://mapr.com/blog/fast-data-processing-pipeline-predicting-flight-delays-using-apache-apis-pt-1/</a><br/>In this lab we will perform one iteration to train on the training data set and test on the test data set.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1552688993761_-84364840","id":"20190308-212752_1210705711","dateCreated":"2019-03-15T22:29:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6251"},{"title":"Train the Model","text":"val model = pipeline.fit(trainingData)","user":"anonymous","dateUpdated":"2019-03-15T22:29:53+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"model: org.apache.spark.ml.PipelineModel = pipeline_5da9e722de12\n"}]},"apps":[],"jobName":"paragraph_1552688993761_1392429337","id":"20171129-113112_994877662","dateCreated":"2019-03-15T22:29:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6252"},{"title":"Print out the feature importances for the random forest model","text":"// Print out the feature importances\nval rfm = model.stages.last.asInstanceOf[RandomForestClassificationModel]\n// we see that the departure time and the orig_dest are the most important features\nval featureImportances =rfm.featureImportances\nassembler.getInputCols.zip(featureImportances.toArray).sortBy(-_._2).foreach { case (feat, imp) => println(s\"feature: $feat, importance: $imp\") }\n\nassembler.getInputCols.zip(featureImportances.toArray).sortBy(-_._2).foreach { case (feat, imp) => println(s\"feature: $feat, importance: $imp\") }\n","user":"anonymous","dateUpdated":"2019-03-15T22:29:53+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"feature: crsdeptime, importance: 0.28351763249788353\nfeature: orig_destIndexed, importance: 0.19366046264846604\nfeature: crsarrtime, importance: 0.18539901941274597\nfeature: crsdephour, importance: 0.11189215100129155\nfeature: dofWIndexed, importance: 0.06079935925478266\nfeature: crselapsedtime, importance: 0.05199848773217375\nfeature: srcIndexed, importance: 0.041716384518939956\nfeature: dstIndexed, importance: 0.03598083086117173\nfeature: dist, importance: 0.019475337697743626\nfeature: carrierIndexed, importance: 0.01556033437480119\nfeature: crsdeptime, importance: 0.28351763249788353\nfeature: orig_destIndexed, importance: 0.19366046264846604\nfeature: crsarrtime, importance: 0.18539901941274597\nfeature: crsdephour, importance: 0.11189215100129155\nfeature: dofWIndexed, importance: 0.06079935925478266\nfeature: crselapsedtime, importance: 0.05199848773217375\nfeature: srcIndexed, importance: 0.041716384518939956\nfeature: dstIndexed, importance: 0.03598083086117173\nfeature: dist, importance: 0.019475337697743626\nfeature: carrierIndexed, importance: 0.01556033437480119\nrfm: org.apache.spark.ml.classification.RandomForestClassificationModel = RandomForestClassificationModel (uid=rfc_9b2dd902708e) with 10 trees\nfeatureImportances: org.apache.spark.ml.linalg.Vector = (10,[0,1,2,3,4,5,6,7,8,9],[0.01556033437480119,0.03598083086117173,0.041716384518939956,0.06079935925478266,0.19366046264846604,0.11189215100129155,0.28351763249788353,0.18539901941274597,0.05199848773217375,0.019475337697743626])\n"}]},"apps":[],"jobName":"paragraph_1552688993762_-1646697261","id":"20171122-232203_359528816","dateCreated":"2019-03-15T22:29:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6253"},{"text":"%md\r\n## Evaluate the model on a test set\r\nThe actual performance of the model can be determined using the test data set which has not been used for any training. We'll transform the test set with the model pipeline, which will transform  the label and features, and then use the random forest model to get the predictions, according to the pipeline steps. \r\n","user":"anonymous","dateUpdated":"2019-03-15T22:29:53+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Evaluate the model on a test set</h2>\n<p>The actual performance of the model can be determined using the test data set which has not been used for any training. We&rsquo;ll transform the test set with the model pipeline, which will transform the label and features, and then use the random forest model to get the predictions, according to the pipeline steps.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1552688993763_-1853351114","id":"20170602-155317_1487132664","dateCreated":"2019-03-15T22:29:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6254"},{"title":"Get Predictions from Test data","text":"//transform the test set with the model pipeline,\n//which will map the features according to the same recipe\nval predictions = model.transform(testData)\n","user":"anonymous","dateUpdated":"2019-03-15T22:29:53+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"predictions: org.apache.spark.sql.DataFrame = [id: string, fldate: string ... 24 more fields]\n"}]},"apps":[],"jobName":"paragraph_1552688993763_-522147882","id":"20170508-155848_1997894070","dateCreated":"2019-03-15T22:29:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6255"},{"text":"%md\nAccuracy is measured by the area under the ROC curve. The area measures the ability of the test to correctly classify true positives from false positives. \nA random predictor would have .5 accuracy. The closer the value is to 1 the better its predictions are. \n\n","user":"anonymous","dateUpdated":"2019-03-15T22:29:53+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Accuracy is measured by the area under the ROC curve. The area measures the ability of the test to correctly classify true positives from false positives.<br/>A random predictor would have .5 accuracy. The closer the value is to 1 the better its predictions are.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1552688993764_-1833291324","id":"20170602-161538_1648758337","dateCreated":"2019-03-15T22:29:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6256"},{"title":"Evaluate the predictions accuracy","text":"val evaluator = new BinaryClassificationEvaluator()  \nval areaUnderROC = evaluator.evaluate(predictions)\nprintln(\"areaUnderROC \"  + areaUnderROC)\n","user":"anonymous","dateUpdated":"2019-03-15T22:29:53+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"areaUnderROC 0.6734984453656299\nevaluator: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_1ede3e4e3497\nareaUnderROC: Double = 0.6734984453656299\n"}]},"apps":[],"jobName":"paragraph_1552688993764_1681823271","id":"20170602-155622_1453197792","dateCreated":"2019-03-15T22:29:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6257"},{"text":"%md\nBelow, we calculate some more metrics. The number of false/true positive and negative predictions is also useful:\n\nTrue positives are how often the model correctly predicted delayed flights.\nFalse positives are how often the model incorrectly predicted delayed flights.\nTrue negatives indicate how often the model correctly predicted not delayed flights.\nFalse negatives indicate how often the model incorrectly predicted not delayed flights.","user":"anonymous","dateUpdated":"2019-03-15T22:29:53+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Below, we calculate some more metrics. The number of false/true positive and negative predictions is also useful:</p>\n<p>True positives are how often the model correctly predicted delayed flights.<br/>False positives are how often the model incorrectly predicted delayed flights.<br/>True negatives indicate how often the model correctly predicted not delayed flights.<br/>False negatives indicate how often the model incorrectly predicted not delayed flights.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1552688993765_1630952123","id":"20190305-233843_231265861","dateCreated":"2019-03-15T22:29:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6258"},{"title":"Calculate some more metrics","text":"val lp = predictions.select(  \"prediction\",\"label\")\nval counttotal = predictions.count().toDouble\nval correct = lp.filter($\"label\" === $\"prediction\").count().toDouble\nval wrong = lp.filter(\"label != prediction\").count().toDouble\nval ratioWrong=wrong/counttotal\nval ratioCorrect=correct/counttotal\nval truen =( lp.filter($\"label\" === 0.0).filter($\"label\" === $\"prediction\").count()) /counttotal\nval truep = (lp.filter($\"label\" === 1.0).filter($\"label\" === $\"prediction\").count())/counttotal\nval falsen = (lp.filter($\"label\" === 0.0).filter(not($\"label\" === $\"prediction\")).count())/counttotal\nval falsep = (lp.filter($\"label\" === 1.0).filter(not($\"label\" === $\"prediction\")).count())/counttotal\n","user":"anonymous","dateUpdated":"2019-03-15T22:29:53+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"lp: org.apache.spark.sql.DataFrame = [prediction: double, label: double]\ncounttotal: Double = 18733.0\ncorrect: Double = 11799.0\nwrong: Double = 6934.0\nratioWrong: Double = 0.3701489350344312\nratioCorrect: Double = 0.6298510649655688\ntruen: Double = 0.34265734265734266\ntruep: Double = 0.28719372230822615\nfalsen: Double = 0.18486094058613142\nfalsep: Double = 0.1852879944482998\n"}]},"apps":[],"jobName":"paragraph_1552688993766_-1001566706","id":"20181015-224231_2027590368","dateCreated":"2019-03-15T22:29:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6259"},{"text":"%md \nHow do you think the accuracy could be improved?\nAdd more historical data? \nAdd more historical data with more features such as such as: holidays, bad weather, airport or airline carrier problems ?\nTune the model with K-fold cross validation ?\n","user":"anonymous","dateUpdated":"2019-03-15T22:29:53+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>How do you think the accuracy could be improved?<br/>Add more historical data?<br/>Add more historical data with more features such as such as: holidays, bad weather, airport or airline carrier problems ?<br/>Tune the model with K-fold cross validation ?</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1552688993766_-635766480","id":"20181004-211413_1357524146","dateCreated":"2019-03-15T22:29:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6260"},{"title":"Save the model to use with streaming data","text":"var dir =\"/user/mapr/model/\"\nmodel.write.overwrite().save(dir)\ndf.unpersist\ndf1.unpersist\ndf2.unpersist\ndf3.unpersist\ntrainingData.unpersist","user":"anonymous","dateUpdated":"2019-03-15T22:29:53+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"dir: String = /user/mapr/model/\nres8: trainingData.type = [id: string, fldate: string ... 14 more fields]\n"}]},"apps":[],"jobName":"paragraph_1552688993767_1850057828","id":"20181015-213933_2083506099","dateCreated":"2019-03-15T22:29:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6261"},{"text":"%md\n## End of Part 2\nThis is the end of this notebook. \nWhen you are finished, make sure you ran the command above to save the model and unpersist the cached DataFrames.\nNext:\n* Go back to Step 2 in Frame on the left and click continue.\n* Follow the instructions on Step 3 before running the next notebook\n* To Run the next notebook, scroll up to the top of this notebook page\n* Click on the Zeppelin Icon at the top of this page\n* This will take you to the list of notebooks \n* Select the notebook FlightDelay3StructuredStreaming","user":"anonymous","dateUpdated":"2019-03-15T22:38:21+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>End of Part 2</h2>\n<p>This is the end of this notebook.<br/>When you are finished, make sure you ran the command above to save the model and unpersist the cached DataFrames.<br/>Next:<br/>* Go back to Step 2 in Frame on the left and click continue.<br/>* Follow the instructions on Step 3 before running the next notebook<br/>* To Run the next notebook, scroll up to the top of this notebook page<br/>* Click on the Zeppelin Icon at the top of this page<br/>* This will take you to the list of notebooks<br/>* Select the notebook FlightDelay3StructuredStreaming</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1552688993767_1016160666","id":"20190308-214104_123247947","dateCreated":"2019-03-15T22:29:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6262","dateFinished":"2019-03-15T22:38:21+0000","dateStarted":"2019-03-15T22:38:21+0000"},{"user":"anonymous","dateUpdated":"2019-03-15T22:29:53+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1552688993767_-1211651768","id":"20190308-214100_1285199507","dateCreated":"2019-03-15T22:29:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6263"},{"user":"anonymous","dateUpdated":"2019-03-15T22:29:53+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1552688993768_303370155","id":"20171122-091021_1615582434","dateCreated":"2019-03-15T22:29:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6264"}],"name":"FlightDelay2SparkMachineLearning","id":"2E885VUE9","noteParams":{},"noteForms":{},"angularObjects":{},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}