{"paragraphs":[{"text":"%md\n# Predicting Flight Delays with Spark ML\nThis notebook demonstrates how to use Apache Spark’s ML pipelines with a Random Forest Classifier to predict flight delays. For more information, see [this](https://mapr.com/ebook/getting-started-with-apache-spark-v2/)  MapR Spark ebook.\n<img src=\"https://mapr.com/blog/datasets-dataframes-and-spark-sql-for-processing-of-tabular-data/assets/image4.jpg\">","user":"mapr","dateUpdated":"2019-03-08T16:44:15+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Predicting Flight Delays with Spark ML</h1>\n<p>This notebook demonstrates how to use Apache Spark’s ML pipelines with a Random Forest Classifier to predict flight delays. For more information, see <a href=\"https://mapr.com/ebook/getting-started-with-apache-spark-v2/\">this</a> MapR Spark ebook.<br/><img src=\"https://mapr.com/blog/datasets-dataframes-and-spark-sql-for-processing-of-tabular-data/assets/image4.jpg\"></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1552060947891_-1546707004","id":"20190220-173254_1728425170","dateCreated":"2019-03-08T16:02:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:13200","dateFinished":"2019-03-08T16:44:15+0000","dateStarted":"2019-03-08T16:44:15+0000"},{"text":"%md \n## Data Exploration\nTo go over some core concepts of Spark Datasets, we will be using some flight information from the United States Department of Transportation. Later, we will use this same data to predict flight delays, so we want to explore the flight attributes that most contribute to flight delays. Using Spark Datasets, we will explore the data to answer questions, like: which airline carriers, days of the week, originating airport, and hours of the day have the highest number of flight delays, when a delay is greater than 40 minutes?\n\nThe flight data is in JSON files, with each flight having the following information:\n\nid: ID composed of origin_destination_date_carrier_flight number\ndofW: day of week (1=Monday, 7=Sunday)\ncarrier: carrier code\nsrc: originating airport code\ndst: destination airport code\ncrsdephour: scheduled departure hour\ncrsdeptime: scheduled departure time\ndepdelay: departure delay in minutes\ncrsarrtime: scheduled arrival time\narrdelay: arrival delay minutes\ncrselapsedtime: elapsed time\ndist: distance\n\nIt appears in the following format:\n{\n    \"id\": \"ATL_BOS_2018-01-01_DL_104\",\n    \"fldate\": \"2018-01-01\",\n    \"month\": 1,\n    \"dofW\": 1,\n    \"carrier\": \"DL\",\n    \"src\": \"ATL\",\n    \"dst\": \"BOS\",\n    \"crsdephour\": 9,\n    \"crsdeptime\": 850,\n    \"depdelay\": 0.0,\n    \"crsarrtime\": 1116,\n    \"arrdelay\": 0.0,\n    \"crselapsedtime\": 146.0,\n    \"dist\": 946.0\n}\n","user":"mapr","dateUpdated":"2019-03-08T16:39:54+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Data Exploration</h2>\n<p>To go over some core concepts of Spark Datasets, we will be using some flight information from the United States Department of Transportation. Later, we will use this same data to predict flight delays, so we want to explore the flight attributes that most contribute to flight delays. Using Spark Datasets, we will explore the data to answer questions, like: which airline carriers, days of the week, originating airport, and hours of the day have the highest number of flight delays, when a delay is greater than 40 minutes?</p>\n<p>The flight data is in JSON files, with each flight having the following information:</p>\n<p>id: ID composed of origin_destination_date_carrier_flight number<br/>dofW: day of week (1=Monday, 7=Sunday)<br/>carrier: carrier code<br/>src: originating airport code<br/>dst: destination airport code<br/>crsdephour: scheduled departure hour<br/>crsdeptime: scheduled departure time<br/>depdelay: departure delay in minutes<br/>crsarrtime: scheduled arrival time<br/>arrdelay: arrival delay minutes<br/>crselapsedtime: elapsed time<br/>dist: distance</p>\n<p>It appears in the following format:<br/>{<br/> &ldquo;id&rdquo;: &ldquo;ATL_BOS_2018-01-01_DL_104&rdquo;,<br/> &ldquo;fldate&rdquo;: &ldquo;2018-01-01&rdquo;,<br/> &ldquo;month&rdquo;: 1,<br/> &ldquo;dofW&rdquo;: 1,<br/> &ldquo;carrier&rdquo;: &ldquo;DL&rdquo;,<br/> &ldquo;src&rdquo;: &ldquo;ATL&rdquo;,<br/> &ldquo;dst&rdquo;: &ldquo;BOS&rdquo;,<br/> &ldquo;crsdephour&rdquo;: 9,<br/> &ldquo;crsdeptime&rdquo;: 850,<br/> &ldquo;depdelay&rdquo;: 0.0,<br/> &ldquo;crsarrtime&rdquo;: 1116,<br/> &ldquo;arrdelay&rdquo;: 0.0,<br/> &ldquo;crselapsedtime&rdquo;: 146.0,<br/> &ldquo;dist&rdquo;: 946.0<br/>}</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1552060947895_1516761955","id":"20170530-122945_1594214131","dateCreated":"2019-03-08T16:02:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13201","dateFinished":"2019-03-08T16:39:54+0000","dateStarted":"2019-03-08T16:39:54+0000"},{"title":"Import needed packages","text":"%spark\nimport org.apache.spark._\nimport org.apache.spark.ml._\nimport org.apache.spark.ml.feature._\nimport org.apache.spark.ml.classification._\nimport org.apache.spark.ml.evaluation._\nimport org.apache.spark.ml.tuning._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._","user":"mapr","dateUpdated":"2019-03-08T16:41:30+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark._\nimport org.apache.spark.ml._\nimport org.apache.spark.ml.feature._\nimport org.apache.spark.ml.classification._\nimport org.apache.spark.ml.evaluation._\nimport org.apache.spark.ml.tuning._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._\n"}]},"apps":[],"jobName":"paragraph_1552060947897_1573407744","id":"20170508-144514_403247535","dateCreated":"2019-03-08T16:02:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13202","dateFinished":"2019-03-08T16:40:32+0000","dateStarted":"2019-03-08T16:40:29+0000"},{"title":"Define Schema for JSON file data","text":"// We use a Scala case class and Spark SQL Structype to define the Dataset schema, \n// corresponding to a line in the JSON data file.\n\ncase class Flight(id: String,fldate: String,month:Integer, dofW: Integer, carrier: String, src: String,dst: String, crsdephour: Integer, crsdeptime: Integer, depdelay: Double, crsarrtime: Integer, arrdelay: Double, crselapsedtime: Double, dist: Double)\n  \n  val schema = StructType(Array(\n    StructField(\"id\", StringType, true),\n    StructField(\"fldate\", StringType, true),\n    StructField(\"month\", IntegerType, true),\n    StructField(\"dofW\", IntegerType, true),\n    StructField(\"carrier\", StringType, true),\n    StructField(\"src\", StringType, true),\n    StructField(\"dst\", StringType, true),\n    StructField(\"crsdephour\", IntegerType, true),\n    StructField(\"crsdeptime\", IntegerType, true),\n    StructField(\"depdelay\", DoubleType, true),\n    StructField(\"crsarrtime\", IntegerType, true),\n    StructField(\"arrdelay\", DoubleType, true),\n    StructField(\"crselapsedtime\", DoubleType, true),\n    StructField(\"dist\", DoubleType, true)\n  ))\n   ","user":"mapr","dateUpdated":"2019-03-08T16:42:03+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"defined class Flight\nschema: org.apache.spark.sql.types.StructType = StructType(StructField(id,StringType,true), StructField(fldate,StringType,true), StructField(month,IntegerType,true), StructField(dofW,IntegerType,true), StructField(carrier,StringType,true), StructField(src,StringType,true), StructField(dst,StringType,true), StructField(crsdephour,IntegerType,true), StructField(crsdeptime,IntegerType,true), StructField(depdelay,DoubleType,true), StructField(crsarrtime,IntegerType,true), StructField(arrdelay,DoubleType,true), StructField(crselapsedtime,DoubleType,true), StructField(dist,DoubleType,true))\n"}]},"apps":[],"jobName":"paragraph_1552060947899_-248155214","id":"20170508-150032_326029627","dateCreated":"2019-03-08T16:02:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13203","dateFinished":"2019-03-08T16:42:04+0000","dateStarted":"2019-03-08T16:42:03+0000"},{"user":"mapr","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1552063372880_-37929817","id":"20190308-164252_1081219350","dateCreated":"2019-03-08T16:42:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:19495","text":"%md\n# Loading Data from the a distributed File into a Dataset\n<img src=\"https://mapr.com/blog/datasets-dataframes-and-spark-sql-for-processing-of-tabular-data/assets/image10.png\">\nA Spark Dataset is a distributed collection of typed objects, which are partitioned across multiple nodes in a cluster and can be operated on in parallel.\nDatasets can be created from MapR-XD files, MapR-DB tables, or MapR-ES topics, and can be cached, allowing reuse across parallel operations. A Dataset can be manipulated using functional transformations (map, flatMap, filter, etc.) and/or Spark SQL. A DataFrame is a Dataset of Row objects and represents a table of data with rows and columns. A DataFrame consists of partitions, each of which is a range of rows in cache on a data node.\n<img src=\"https://mapr.com/blog/datasets-dataframes-and-spark-sql-for-processing-of-tabular-data/assets/image12.png\">\nThe entry point to programming in Spark is the org.apache.spark.sql.SparkSession class\nIf you are using the spark-shell or a notebook, the SparkSession object is already created and available as the variable spark.\nWith the SparkSession read method, we can read data from a file into a DataFrame, specifying the file type, file path, and input options for the schema. \nThe schema can optionally be inferred from the contents of the JSON file, but you will get better performance and accuracy by specifying the schema.\n\n","dateUpdated":"2019-03-08T17:15:53+0000","dateFinished":"2019-03-08T17:02:42+0000","dateStarted":"2019-03-08T17:02:42+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Loading Data from the a distributed File into a Dataset</h1>\n<img src=\"https://mapr.com/blog/datasets-dataframes-and-spark-sql-for-processing-of-tabular-data/assets/image10.png\">\n<p>A Spark Dataset is a distributed collection of typed objects, which are partitioned across multiple nodes in a cluster and can be operated on in parallel.<br/>Datasets can be created from MapR-XD files, MapR-DB tables, or MapR-ES topics, and can be cached, allowing reuse across parallel operations. A Dataset can be manipulated using functional transformations (map, flatMap, filter, etc.) and/or Spark SQL. A DataFrame is a Dataset of Row objects and represents a table of data with rows and columns. A DataFrame consists of partitions, each of which is a range of rows in cache on a data node.<br/><img src=\"https://mapr.com/blog/datasets-dataframes-and-spark-sql-for-processing-of-tabular-data/assets/image12.png\"><br/>The entry point to programming in Spark is the org.apache.spark.sql.SparkSession class<br/>If you are using the spark-shell or a notebook, the SparkSession object is already created and available as the variable spark.<br/>With the SparkSession read method, we can read data from a file into a DataFrame, specifying the file type, file path, and input options for the schema.<br/>The schema can optionally be inferred from the contents of the JSON file, but you will get better performance and accuracy by specifying the schema.</p>\n</div>"}]}},{"title":"Read the data from JSON file into a Dataset of type Flight","text":"import spark.implicits._\n\n// file path location\n var file =\"/user/mapr/data/flightdata2018.json\"\n// With the SparkSession read method, read data from a file into a Dataset\n val df: Dataset[Flight] = spark.read.format(\"json\").option(\"inferSchema\", \"false\").schema(schema).load(file).as[Flight]\n// display the first 20 rows of the DataFrame\n df.show\n","user":"mapr","dateUpdated":"2019-03-08T16:59:42+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import spark.implicits._\nfile: String = /user/mapr/data/flightdata2018.json\ndf: org.apache.spark.sql.Dataset[Flight] = [id: string, fldate: string ... 12 more fields]\n+--------------------+----------+-----+----+-------+---+---+----------+----------+--------+----------+--------+--------------+-----+\n|                  id|    fldate|month|dofW|carrier|src|dst|crsdephour|crsdeptime|depdelay|crsarrtime|arrdelay|crselapsedtime| dist|\n+--------------------+----------+-----+----+-------+---+---+----------+----------+--------+----------+--------+--------------+-----+\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|BOS|         9|       850|     0.0|      1116|     0.0|         146.0|946.0|\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|BOS|        11|      1122|     8.0|      1349|     0.0|         147.0|946.0|\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|BOS|        14|      1356|     9.0|      1623|     0.0|         147.0|946.0|\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|BOS|        16|      1620|     0.0|      1851|     3.0|         151.0|946.0|\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|BOS|        19|      1940|     6.0|      2210|     0.0|         150.0|946.0|\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|BOS|        12|      1248|     0.0|      1513|     0.0|         145.0|946.0|\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|BOS|        22|      2215|     0.0|        39|     0.0|         144.0|946.0|\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|BOS|        15|      1500|    21.0|      1734|    33.0|         154.0|946.0|\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     WN|ATL|BOS|        15|      1500|   198.0|      1725|   208.0|         145.0|946.0|\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     WN|ATL|BOS|        21|      2055|    14.0|      2330|     0.0|         155.0|946.0|\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     WN|ATL|BOS|        10|      1015|   215.0|      1250|   191.0|         155.0|946.0|\n|ATL_CLT_2018-01-0...|2018-01-01|    1|   1|     AA|ATL|CLT|        11|      1114|     0.0|      1238|     0.0|          84.0|226.0|\n|ATL_CLT_2018-01-0...|2018-01-01|    1|   1|     AA|ATL|CLT|         8|       845|     0.0|      1011|     0.0|          86.0|226.0|\n|ATL_CLT_2018-01-0...|2018-01-01|    1|   1|     AA|ATL|CLT|        15|      1548|     0.0|      1710|     0.0|          82.0|226.0|\n|ATL_CLT_2018-01-0...|2018-01-01|    1|   1|     AA|ATL|CLT|         7|       705|     0.0|       821|     0.0|          76.0|226.0|\n|ATL_CLT_2018-01-0...|2018-01-01|    1|   1|     AA|ATL|CLT|        12|      1226|     0.0|      1347|     0.0|          81.0|226.0|\n|ATL_CLT_2018-01-0...|2018-01-01|    1|   1|     AA|ATL|CLT|        22|      2205|     0.0|      2319|     1.0|          74.0|226.0|\n|ATL_CLT_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|CLT|        22|      2210|    11.0|      2324|     0.0|          74.0|226.0|\n|ATL_CLT_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|CLT|        15|      1543|     1.0|      1659|     0.0|          76.0|226.0|\n|ATL_CLT_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|CLT|        10|      1008|     0.0|      1124|     0.0|          76.0|226.0|\n+--------------------+----------+-----+----+-------+---+---+----------+----------+--------+----------+--------+--------------+-----+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1552060947900_2057789070","id":"20170508-150131_378637203","dateCreated":"2019-03-08T16:02:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13204","dateFinished":"2019-03-08T16:46:22+0000","dateStarted":"2019-03-08T16:46:16+0000","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://mapr-resman:8088/proxy/application_1551978809362_0002//jobs"],"interpreterSettingId":"spark"}}},{"user":"mapr","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1552063790409_-294218351","id":"20190308-164950_1853207952","dateCreated":"2019-03-08T16:49:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:19713","text":"// If we supply a scala case class with the as method when loading the data, \n// then the data is read into a Dataset of typed objects corresponding to the case class.\n// the take 1 method returns an Array with 1 flight object\ndf.take(1)","dateUpdated":"2019-03-08T16:53:02+0000","dateFinished":"2019-03-08T16:50:17+0000","dateStarted":"2019-03-08T16:50:14+0000","title":" Take method on Dataset returns an array of Flight objects.","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res66: Array[Flight] = Array(Flight(ATL_BOS_2018-01-01_DL_104,2018-01-01,1,1,DL,ATL,BOS,9,850,0.0,1116,0.0,146.0,946.0))\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://mapr-resman:8088/proxy/application_1551978809362_0002//jobs"],"interpreterSettingId":"spark"}}},{"title":"Print DataFrame Schema","text":"// printSchema method prints out the dataFrame schmea \ndf.printSchema","user":"mapr","dateUpdated":"2019-03-08T17:06:14+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- id: string (nullable = true)\n |-- fldate: string (nullable = true)\n |-- month: integer (nullable = true)\n |-- dofW: integer (nullable = true)\n |-- carrier: string (nullable = true)\n |-- src: string (nullable = true)\n |-- dst: string (nullable = true)\n |-- crsdephour: integer (nullable = true)\n |-- crsdeptime: integer (nullable = true)\n |-- depdelay: double (nullable = true)\n |-- crsarrtime: integer (nullable = true)\n |-- arrdelay: double (nullable = true)\n |-- crselapsedtime: double (nullable = true)\n |-- dist: double (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1552060947904_1033838456","id":"20171129-223643_463511351","dateCreated":"2019-03-08T16:02:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13206","dateFinished":"2019-03-08T17:06:15+0000","dateStarted":"2019-03-08T17:06:14+0000"},{"text":"%md\nThe describe() function performs summary statistics calculations on  numeric columns ","user":"mapr","dateUpdated":"2019-03-08T17:06:23+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>The describe() function performs summary statistics calculations on numeric columns</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1552060947906_-1460400728","id":"20170524-214640_973339640","dateCreated":"2019-03-08T16:02:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13207","dateFinished":"2019-03-08T17:06:23+0000","dateStarted":"2019-03-08T17:06:23+0000"},{"title":"Perform summary statistics  on selected columns","text":"\ndf.describe(\"dist\", \"crselapsedtime\",\"depdelay\", \"arrdelay\").show","user":"mapr","dateUpdated":"2019-03-08T17:06:37+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+------------------+------------------+------------------+------------------+\n|summary|              dist|    crselapsedtime|          depdelay|          arrdelay|\n+-------+------------------+------------------+------------------+------------------+\n|  count|            282628|            282628|            282628|            282628|\n|   mean|1154.9526267744172|189.82255119804125|14.468792900915691|14.565644592892424|\n| stddev| 629.5585331710331| 75.74564106190759| 43.72836248563901| 43.47539843809362|\n|    min|             184.0|              61.0|               0.0|               0.0|\n|    max|            2724.0|             433.0|            1559.0|            1576.0|\n+-------+------------------+------------------+------------------+------------------+\n\n"}]},"apps":[],"jobName":"paragraph_1552060947907_-323548913","id":"20170524-083228_1459810795","dateCreated":"2019-03-08T16:02:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13208","dateFinished":"2019-03-08T17:06:40+0000","dateStarted":"2019-03-08T17:06:37+0000","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://mapr-resman:8088/proxy/application_1551978809362_0002//jobs"],"interpreterSettingId":"spark"}}},{"user":"mapr","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1552065120537_1836412768","id":"20190308-171200_853490725","dateCreated":"2019-03-08T17:12:00+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:20389","text":"%md\n# Transformations and Actions\nThere are two types of operations you can perform on a Dataset:\n\ntransformations: create a new Dataset from the current Dataset\nactions: trigger computation and return a result to the driver program\n<img src=\"https://mapr.com/blog/datasets-dataframes-and-spark-sql-for-processing-of-tabular-data/assets/image1.png\">\nDataFrame Transformations\nHere is a list of some commonly used untyped transformations, which can be used on Dataframes (Dataset[Row]).\n\nTransformation\tDescription\nselect\tSelects a set of columns\njoin\tJoin with another DataFrame, using the given join expression\ngroupBy\tGroups the DataFrame, using the specified columns\n\nHere is a list of some commonly used Dataset actions.\nAction\tDescription\nshow(n)\tDisplays the first n rows in a tabular form\ntake(n)\tReturns the first n objects in the Dataset in an array\ncount\tReturns the number of rows in the Dataset\n","dateUpdated":"2019-03-08T17:16:10+0000","dateFinished":"2019-03-08T17:16:10+0000","dateStarted":"2019-03-08T17:16:10+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Transformations and Actions</h1>\n<p>There are two types of operations you can perform on a Dataset:</p>\n<p>transformations: create a new Dataset from the current Dataset<br/>actions: trigger computation and return a result to the driver program<br/><img src=\"https://mapr.com/blog/datasets-dataframes-and-spark-sql-for-processing-of-tabular-data/assets/image1.png\"><br/>DataFrame Transformations<br/>Here is a list of some commonly used untyped transformations, which can be used on Dataframes (Dataset[Row]).</p>\n<p>Transformation Description<br/>select Selects a set of columns<br/>join Join with another DataFrame, using the given join expression<br/>groupBy Groups the DataFrame, using the specified columns</p>\n<p>Here is a list of some commonly used Dataset actions.<br/>Action Description<br/>show(n) Displays the first n rows in a tabular form<br/>take(n) Returns the first n objects in the Dataset in an array<br/>count Returns the number of rows in the Dataset</p>\n</div>"}]}},{"text":"// Here is an example using typed and untyped transformations and actions to get the originating airports \n// with the highest number of departure delays, where a delay is greater than 40 minutes. \n// We count the departure delays greater than 40 minutes by src and sort them with the highest first.\ndf.filter($\"depdelay\" > 40).groupBy(\"src\").count().orderBy(desc(\"count\")).show","user":"mapr","dateUpdated":"2019-03-08T17:15:47+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1552065219839_-834072039","id":"20190308-171339_1735802559","dateCreated":"2019-03-08T17:13:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:20465","dateFinished":"2019-03-08T17:14:38+0000","dateStarted":"2019-03-08T17:14:35+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+-----+\n|src|count|\n+---+-----+\n|ORD| 4033|\n|ATL| 3106|\n|DFW| 2782|\n|EWR| 2328|\n|DEN| 2304|\n|MIA| 2294|\n|SFO| 2261|\n|LAX| 2126|\n|LGA| 1944|\n|IAH| 1829|\n|CLT| 1755|\n|BOS| 1711|\n|SEA|  846|\n+---+-----+\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://mapr-resman:8088/proxy/application_1551978809362_0002//jobs"],"interpreterSettingId":"spark"}}},{"text":"%md\nIn the code below a Spark Bucketizer is used to split the dataset into delayed and not delayed flights (where delayed > 40 minutes ) \nwith a delayed 0/1 column. Then the resulting total counts are displayed. ","user":"mapr","dateUpdated":"2019-03-08T17:17:21+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>In the code below a Spark Bucketizer is used to split the dataset into delayed and not delayed flights (where delayed &gt; 40 minutes )<br/>with a delayed 0/1 column. Then the resulting total counts are displayed.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1552060947909_2069362344","id":"20190305-221927_113621865","dateCreated":"2019-03-08T16:02:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13209","dateFinished":"2019-03-08T17:17:21+0000","dateStarted":"2019-03-08T17:17:21+0000"},{"title":"Add Column for Delayed Flights and count","text":"// bucket > 40 minutes = delayed\nval delaybucketizer = new Bucketizer().setInputCol(\"depdelay\").setOutputCol(\"delayed\").setSplits(Array(0.0,41.0,Double.PositiveInfinity))\nval df1= delaybucketizer.transform(df)\ndf1.cache\ndf1.groupBy(\"delayed\").count.show","user":"mapr","dateUpdated":"2019-03-08T17:07:37+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"delaybucketizer: org.apache.spark.ml.feature.Bucketizer = bucketizer_28ee0c22003c\ndf1: org.apache.spark.sql.DataFrame = [id: string, fldate: string ... 13 more fields]\nres78: df1.type = [id: string, fldate: string ... 13 more fields]\n+-------+------+\n|delayed| count|\n+-------+------+\n|    0.0|253309|\n|    1.0| 29319|\n+-------+------+\n\n"}]},"apps":[],"jobName":"paragraph_1552060947911_-870707347","id":"20170524-093402_1430077788","dateCreated":"2019-03-08T16:02:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13210","dateFinished":"2019-03-08T17:07:44+0000","dateStarted":"2019-03-08T17:07:37+0000","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://mapr-resman:8088/proxy/application_1551978809362_0002//jobs","http://mapr-resman:8088/proxy/application_1551978809362_0002//jobs","http://mapr-resman:8088/proxy/application_1551978809362_0002//jobs","http://mapr-resman:8088/proxy/application_1551978809362_0002//jobs","http://mapr-resman:8088/proxy/application_1551978809362_0002//jobs"],"interpreterSettingId":"spark"}}},{"text":"%md \n# Exploring the Flight Dataset with Spark SQL\nNow let’s explore the flight Dataset using Spark SQL and DataFrame transformations. After we register the DataFrame as a SQL temporary view, we can use SQL functions on the SparkSession to run SQL queries, which will return the results as a DataFrame. We cache the DataFrame, since we will reuse it and because Spark can cache DataFrames or Tables in columnar format in memory, which can improve memory usage and performance.","user":"mapr","dateUpdated":"2019-03-08T17:18:18+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Exploring the Flight Dataset with Spark SQL</h1>\n<p>Now let’s explore the flight Dataset using Spark SQL and DataFrame transformations. After we register the DataFrame as a SQL temporary view, we can use SQL functions on the SparkSession to run SQL queries, which will return the results as a DataFrame. We cache the DataFrame, since we will reuse it and because Spark can cache DataFrames or Tables in columnar format in memory, which can improve memory usage and performance.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1552060947914_-1373646507","id":"20170603-182655_1680505289","dateCreated":"2019-03-08T16:02:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13212","dateFinished":"2019-03-08T17:18:18+0000","dateStarted":"2019-03-08T17:18:18+0000","focus":true},{"title":"Register Dataset as a Temporary View in order to explore with SQL","text":"df1.createOrReplaceTempView(\"flights\")\ndf1.cache\n","user":"mapr","dateUpdated":"2019-03-08T17:18:35+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":true,"setting":{"stackedAreaChart":{}},"commonSetting":{},"keys":[{"name":"churn","index":19,"aggr":"sum"}],"groups":[],"values":[{"name":"len","index":1,"aggr":"sum"}]},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res88: df1.type = [id: string, fldate: string ... 13 more fields]\n"}]},"apps":[],"jobName":"paragraph_1552060947912_-1359662772","id":"20170508-150408_505244914","dateCreated":"2019-03-08T16:02:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13211","dateFinished":"2019-03-08T17:18:37+0000","dateStarted":"2019-03-08T17:18:35+0000"},{"user":"mapr","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1552065555828_248569115","id":"20190308-171915_1304323195","dateCreated":"2019-03-08T17:19:15+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:20809","text":"//Below, we display information for the top five longest departure delays with DataFrame transformations\ndf1.select($\"carrier\",$\"src\",$\"dst\",$\"depdelay\", $\"crsdephour\").filter($\"depdelay\" > 40).orderBy(desc( \"depdelay\" )).show(5)","dateUpdated":"2019-03-08T17:21:51+0000","dateFinished":"2019-03-08T17:19:59+0000","dateStarted":"2019-03-08T17:19:57+0000","title":"Top 5 Longest departure delays with DataFrame transformations","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+---+---+--------+----------+\n|carrier|src|dst|depdelay|crsdephour|\n+-------+---+---+--------+----------+\n|     AA|IAH|MIA|  1559.0|        11|\n|     AA|IAH|DFW|  1445.0|        18|\n|     AA|BOS|DFW|  1345.0|        10|\n|     UA|BOS|ORD|  1334.0|         9|\n|     AA|LAX|MIA|  1283.0|         9|\n+-------+---+---+--------+----------+\nonly showing top 5 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://mapr-resman:8088/proxy/application_1551978809362_0002//jobs"],"interpreterSettingId":"spark"}}},{"title":"Top 5 Longest departure delays with SQL ","text":"%sql \nselect carrier,src, dst, depdelay,crsdephour, dist, dofW\nfrom flights where depdelay > 40\norder by depdelay desc limit 5 \n","user":"mapr","dateUpdated":"2019-03-08T17:21:37+0000","config":{"tableHide":false,"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","editorHide":false,"fontSize":9,"title":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"carrier":"string","src":"string","dst":"string","depdelay":"string","crsdephour":"string","dist":"string","dofW":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"carrier\tsrc\tdst\tdepdelay\tcrsdephour\tdist\tdofW\nAA\tIAH\tMIA\t1559.0\t11\t964.0\t5\nAA\tIAH\tDFW\t1445.0\t18\t224.0\t3\nAA\tBOS\tDFW\t1345.0\t10\t1562.0\t4\nUA\tBOS\tORD\t1334.0\t9\t867.0\t4\nAA\tLAX\tMIA\t1283.0\t9\t2342.0\t1\n"}]},"apps":[],"jobName":"paragraph_1552060947916_755329476","id":"20171110-195321_1924356975","dateCreated":"2019-03-08T16:02:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13213","dateFinished":"2019-03-08T17:21:19+0000","dateStarted":"2019-03-08T17:21:17+0000","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://mapr-resman:8088/proxy/application_1551978809362_0002//jobs"],"interpreterSettingId":"spark"}}},{"title":"Average Departure Delay by Carrier","text":"%sql select carrier, avg(depdelay)\nfrom flights\ngroup by carrier","user":"mapr","dateUpdated":"2019-03-08T16:08:23+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"editorHide":false,"title":true,"results":{"0":{"graph":{"mode":"multiBarChart","height":300,"optionOpen":false,"setting":{"multiBarChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"}},"commonSetting":{},"keys":[{"name":"carrier","index":0,"aggr":"sum"}],"groups":[],"values":[{"name":"avg(depdelay)","index":1,"aggr":"sum"}]},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"carrier\tavg(depdelay)\nUA\t14.442583200307139\nAA\t14.129090626627798\nDL\t14.26168914855228\nWN\t17.70183428999251\n"},{"type":"TEXT","data":""}]},"apps":[],"jobName":"paragraph_1552060947917_-354014296","id":"20171110-195759_439800395","dateCreated":"2019-03-08T16:02:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13214"},{"title":"Average Departure Delay by Destination Airport","text":"%sql\nSELECT dst, avg(depdelay) as avgdelay\nFROM flights \nGROUP BY dst\nORDER BY avgdelay desc","user":"mapr","dateUpdated":"2019-03-08T16:08:23+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"editorHide":false,"title":true,"results":{"0":{"graph":{"mode":"multiBarChart","height":300,"optionOpen":false,"setting":{"multiBarChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"},"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"dst":"string","avgdelay":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{},"keys":[{"name":"dst","index":0,"aggr":"sum"}],"groups":[],"values":[{"name":"avgdelay","index":1,"aggr":"sum"}]},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"dst\tavgdelay\nEWR\t20.729607250755286\nSFO\t17.047248322147652\nLGA\t15.613878829401978\nBOS\t15.00700909577314\nORD\t14.846011016896702\nMIA\t14.371604732544576\nDFW\t13.946526636469315\nATL\t13.797100969652844\nLAX\t13.445941419479698\nSEA\t13.097595473833097\nDEN\t12.767347642444618\nIAH\t11.743971267316573\nCLT\t11.370676826001548\n"},{"type":"TEXT","data":""}]},"apps":[],"jobName":"paragraph_1552060947919_-1507231456","id":"20171122-204956_303346727","dateCreated":"2019-03-08T16:02:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13215"},{"title":"Average Departure Delay by Originating Airport","text":"%sql\nSELECT src, avg(depdelay) as avgdelay\nFROM flights \nGROUP BY src\nORDER BY avgdelay desc","user":"mapr","dateUpdated":"2019-03-08T16:08:23+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"editorHide":false,"title":true,"results":{"0":{"graph":{"mode":"multiBarChart","height":300,"optionOpen":false,"setting":{"multiBarChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"}},"commonSetting":{},"keys":[{"name":"src","index":0,"aggr":"sum"}],"groups":[],"values":[{"name":"avgdelay","index":1,"aggr":"sum"}]},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"src\tavgdelay\nEWR\t17.818079459546404\nMIA\t17.768691978431264\nORD\t16.5199551010227\nATL\t15.330084535057185\nDFW\t15.061909338459074\nDEN\t14.192011960700555\nCLT\t14.081517835767297\nIAH\t13.804911168948738\nSFO\t13.792674746209919\nLGA\t13.586563812127721\nBOS\t12.940644675240812\nLAX\t11.223347230494342\nSEA\t10.140899425061038\n"},{"type":"TEXT","data":""}]},"apps":[],"jobName":"paragraph_1552060947921_-1087670761","id":"20171122-204822_1853914961","dateCreated":"2019-03-08T16:02:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13216"},{"title":"Count of Departure Delays by Day of the Week (1=Monday, 7=Sunday)","text":"%sql\nselect dofW, count(depdelay)\nfrom flights where depdelay > 40\ngroup by dofW\n","user":"mapr","dateUpdated":"2019-03-08T16:08:23+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"editorHide":false,"title":true,"results":{"0":{"graph":{"mode":"multiBarChart","height":300,"optionOpen":false,"setting":{"multiBarChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"}},"commonSetting":{},"keys":[{"name":"dofW","index":0,"aggr":"sum"}],"groups":[],"values":[{"name":"count(depdelay)","index":1,"aggr":"sum"}]},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"dofW\tcount(depdelay)\n1\t4680\n6\t2598\n3\t3986\n5\t4677\n4\t4809\n7\t4163\n2\t4406\n"},{"type":"TEXT","data":""}]},"apps":[],"jobName":"paragraph_1552060947922_-702608044","id":"20171123-101016_2040441762","dateCreated":"2019-03-08T16:02:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13217"},{"title":"Count of Departure Delays by Carrier (where delay=40 minutes)","text":"%sql\nselect carrier, count(delayed)\nfrom flights where delayed = 1\ngroup by carrier\norder by carrier","user":"mapr","dateUpdated":"2019-03-08T16:08:23+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"editorHide":false,"title":true,"results":{"0":{"graph":{"mode":"multiBarChart","height":300,"optionOpen":false,"setting":{"multiBarChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"}},"commonSetting":{},"keys":[{"name":"carrier","index":0,"aggr":"sum"}],"groups":[],"values":[{"name":"count(delayed)","index":1,"aggr":"sum"}]},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"carrier\tcount(delayed)\nAA\t11570\nDL\t5354\nUA\t10297\nWN\t2098\n"},{"type":"TEXT","data":""}]},"apps":[],"jobName":"paragraph_1552060947924_-839269781","id":"20171123-042011_1532799341","dateCreated":"2019-03-08T16:02:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13218"},{"title":"Count of Delayed Not Delayed  by Carrier","text":"%sql\nSELECT delayed, count(delayed),carrier\nFROM flights\nGROUP BY carrier, delayed\norder by carrier\n","user":"mapr","dateUpdated":"2019-03-08T16:08:24+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"editorHide":false,"title":true,"results":{"0":{"graph":{"mode":"multiBarChart","height":300,"optionOpen":false,"setting":{"multiBarChart":{"stacked":false,"rotate":{"degree":"-45"},"xLabelStatus":"default"}},"commonSetting":{},"keys":[{"name":"carrier","index":2,"aggr":"sum"}],"groups":[{"name":"delayed","index":0,"aggr":"sum"}],"values":[{"name":"count(delayed)","index":1,"aggr":"sum"}]},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"delayed\tcount(delayed)\tcarrier\n1.0\t11570\tAA\n0.0\t97857\tAA\n0.0\t52841\tDL\n1.0\t5354\tDL\n0.0\t88681\tUA\n1.0\t10297\tUA\n1.0\t2098\tWN\n0.0\t13930\tWN\n"},{"type":"TEXT","data":""}]},"apps":[],"jobName":"paragraph_1552060947925_1387658132","id":"20170524-144751_554129160","dateCreated":"2019-03-08T16:02:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13219"},{"title":"Count of Departure Delays by Hour of Day","text":"%sql\nselect crsdephour, count(depdelay)\nfrom flights where depdelay > 40\ngroup by crsdephour\n","user":"mapr","dateUpdated":"2019-03-08T16:08:24+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"editorHide":false,"title":true,"results":{"0":{"graph":{"mode":"multiBarChart","height":300,"optionOpen":false,"setting":{"multiBarChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"}},"commonSetting":{},"keys":[{"name":"crsdephour","index":0,"aggr":"sum"}],"groups":[],"values":[{"name":"count(depdelay)","index":1,"aggr":"sum"}]},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"crsdephour\tcount(depdelay)\n12\t1893\n22\t1066\n1\t51\n13\t1728\n6\t518\n16\t2309\n20\t2260\n5\t175\n19\t2198\n15\t1969\n17\t2434\n9\t1112\n8\t1067\n23\t364\n7\t1068\n10\t1472\n24\t58\n21\t1180\n11\t1321\n14\t2041\n0\t106\n18\t2929\n"},{"type":"TEXT","data":""}]},"apps":[],"jobName":"paragraph_1552060947927_-1194476238","id":"20171129-200107_756396508","dateCreated":"2019-03-08T16:02:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13220"},{"title":"Count of Delayed , Not Delayed by Hour","text":"%sql\nSELECT delayed, count(delayed),crsdephour\nFROM flights\nGROUP BY crsdephour, delayed\norder by delayed","user":"mapr","dateUpdated":"2019-03-08T16:08:24+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"editorHide":false,"title":true,"results":{"0":{"graph":{"mode":"multiBarChart","height":300,"optionOpen":false,"setting":{"multiBarChart":{"stacked":false,"rotate":{"degree":"-45"},"xLabelStatus":"default"}},"commonSetting":{},"keys":[{"name":"crsdephour","index":2,"aggr":"sum"}],"groups":[{"name":"delayed","index":0,"aggr":"sum"}],"values":[{"name":"count(delayed)","index":1,"aggr":"sum"}]},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"delayed\tcount(delayed)\tcrsdephour\n0.0\t22365\t7\n0.0\t1499\t0\n0.0\t7242\t22\n0.0\t10675\t20\n0.0\t1217\t1\n0.0\t14966\t18\n0.0\t17327\t10\n0.0\t6511\t21\n0.0\t16790\t12\n0.0\t17001\t8\n0.0\t1017\t24\n0.0\t16660\t6\n0.0\t14946\t9\n0.0\t18\t2\n0.0\t14940\t11\n0.0\t2705\t23\n0.0\t13895\t13\n0.0\t15123\t14\n0.0\t14661\t16\n0.0\t13302\t15\n0.0\t10871\t19\n0.0\t13488\t17\n0.0\t6090\t5\n1.0\t1893\t12\n1.0\t364\t23\n1.0\t1321\t11\n1.0\t2260\t20\n1.0\t2434\t17\n1.0\t1112\t9\n1.0\t2198\t19\n1.0\t2041\t14\n1.0\t1728\t13\n1.0\t175\t5\n1.0\t58\t24\n1.0\t2929\t18\n1.0\t51\t1\n1.0\t1180\t21\n1.0\t1067\t8\n1.0\t1969\t15\n1.0\t1066\t22\n1.0\t1472\t10\n1.0\t1068\t7\n1.0\t2309\t16\n1.0\t518\t6\n1.0\t106\t0\n"},{"type":"TEXT","data":""}]},"apps":[],"jobName":"paragraph_1552060947928_1415326022","id":"20170524-095127_757444423","dateCreated":"2019-03-08T16:02:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13221"},{"title":"Count of Departure Delays by Origin","text":"%sql\nSELECT delayed, count(delayed),src\nFROM flights\nGROUP BY src, delayed\norder by  src\n","user":"mapr","dateUpdated":"2019-03-08T16:08:24+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"editorHide":false,"title":true,"results":{"0":{"graph":{"mode":"multiBarChart","height":300,"optionOpen":false,"setting":{"pieChart":{},"multiBarChart":{"stacked":false,"rotate":{"degree":"-45"},"xLabelStatus":"default"}},"commonSetting":{},"keys":[{"name":"src","index":2,"aggr":"sum"}],"groups":[{"name":"delayed","index":0,"aggr":"sum"}],"values":[{"name":"count(delayed)","index":1,"aggr":"sum"}]},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"delayed\tcount(delayed)\tsrc\n1.0\t3106\tATL\n0.0\t27059\tATL\n1.0\t1711\tBOS\n0.0\t16872\tBOS\n1.0\t1755\tCLT\n0.0\t14953\tCLT\n1.0\t2304\tDEN\n0.0\t21106\tDEN\n0.0\t22190\tDFW\n1.0\t2782\tDFW\n1.0\t2328\tEWR\n0.0\t16323\tEWR\n0.0\t15845\tIAH\n1.0\t1829\tIAH\n1.0\t2126\tLAX\n0.0\t24738\tLAX\n0.0\t18538\tLGA\n1.0\t1944\tLGA\n1.0\t2294\tMIA\n0.0\t15695\tMIA\n0.0\t28039\tORD\n1.0\t4033\tORD\n0.0\t11851\tSEA\n1.0\t846\tSEA\n1.0\t2261\tSFO\n0.0\t20100\tSFO\n"},{"type":"TEXT","data":""}]},"apps":[],"jobName":"paragraph_1552060947930_1326816909","id":"20171110-211045_792848918","dateCreated":"2019-03-08T16:02:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13222"},{"title":"Count of Delayed not Delayed by Destination","text":"%sql\nSELECT delayed, count(delayed),dst\nFROM flights\nGROUP BY dst, delayed\norder by dst","user":"mapr","dateUpdated":"2019-03-08T16:08:24+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"editorHide":false,"title":true,"results":{"0":{"graph":{"mode":"multiBarChart","height":300,"optionOpen":false,"setting":{"multiBarChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"}},"commonSetting":{},"keys":[{"name":"dst","index":2,"aggr":"sum"}],"groups":[{"name":"delayed","index":0,"aggr":"sum"}],"values":[{"name":"count(delayed)","index":1,"aggr":"sum"}]},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"delayed\tcount(delayed)\tdst\n1.0\t2759\tATL\n0.0\t27458\tATL\n0.0\t16612\tBOS\n1.0\t2078\tBOS\n1.0\t1390\tCLT\n0.0\t15409\tCLT\n1.0\t2171\tDEN\n0.0\t21031\tDEN\n0.0\t22476\tDFW\n1.0\t2471\tDFW\n0.0\t15740\tEWR\n1.0\t2796\tEWR\n1.0\t1468\tIAH\n0.0\t16073\tIAH\n0.0\t24389\tLAX\n1.0\t2480\tLAX\n1.0\t2363\tLGA\n0.0\t18071\tLGA\n0.0\t16262\tMIA\n1.0\t1741\tMIA\n1.0\t3465\tORD\n0.0\t28849\tORD\n0.0\t11535\tSEA\n1.0\t1191\tSEA\n1.0\t2946\tSFO\n0.0\t19404\tSFO\n"},{"type":"TEXT","data":""}]},"apps":[],"jobName":"paragraph_1552060947932_-1000814089","id":"20171122-211508_2036968500","dateCreated":"2019-03-08T16:02:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13223"},{"title":"Count of Departure Delays by Origin, Destination","text":"%sql\nselect src, dst, count(depdelay)\nfrom flights where depdelay > 40\ngroup by src, dst\nORDER BY count(depdelay) desc\n","user":"mapr","dateUpdated":"2019-03-08T17:04:37+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"editorHide":false,"title":true,"results":{"0":{"graph":{"mode":"multiBarChart","height":300,"optionOpen":false,"setting":{"multiBarChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"}},"commonSetting":{},"keys":[{"name":"src","index":0,"aggr":"sum"}],"groups":[{"name":"dst","index":1,"aggr":"sum"}],"values":[{"name":"count(depdelay)","index":2,"aggr":"sum"}]},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"src\tdst\tcount(depdelay)\nORD\tLGA\t588\nLAX\tSFO\t578\nATL\tEWR\t561\nLGA\tORD\t532\nORD\tSFO\t470\nSFO\tLAX\t463\nATL\tLGA\t445\nORD\tLAX\t425\nORD\tDFW\t423\nDEN\tSFO\t388\nEWR\tATL\t381\nDFW\tORD\t377\nMIA\tLGA\t355\nORD\tEWR\t347\nLGA\tATL\t344\nSFO\tORD\t343\nDFW\tATL\t328\nORD\tATL\t326\nORD\tBOS\t318\nDEN\tLAX\t311\nLAX\tDEN\t303\nORD\tDEN\t302\nSFO\tDEN\t299\nMIA\tATL\t299\nLAX\tORD\t298\nEWR\tORD\t297\nATL\tDFW\t293\nBOS\tLGA\t279\nLGA\tBOS\t276\nDEN\tEWR\t275\nATL\tLAX\t275\nATL\tORD\t273\nBOS\tEWR\t269\nDFW\tSFO\t269\nIAH\tEWR\t269\nBOS\tORD\t267\nDEN\tORD\t265\nIAH\tDFW\t264\nMIA\tORD\t262\nMIA\tDFW\t259\nEWR\tSFO\t257\nATL\tMIA\t255\nEWR\tBOS\t255\nCLT\tEWR\t255\nDFW\tIAH\t255\nLGA\tMIA\t248\nATL\tBOS\t243\nORD\tSEA\t232\nDFW\tDEN\t222\nORD\tIAH\t217\nDFW\tMIA\t217\nDFW\tLAX\t216\nDFW\tLGA\t214\nSFO\tEWR\t214\nCLT\tORD\t213\nORD\tMIA\t212\nEWR\tLAX\t202\nSFO\tDFW\t202\nLAX\tATL\t201\nMIA\tEWR\t201\nDFW\tEWR\t200\nIAH\tSFO\t199\nDEN\tDFW\t198\nMIA\tLAX\t198\nBOS\tATL\t198\nSFO\tSEA\t196\nATL\tDEN\t192\nSEA\tSFO\t191\nIAH\tORD\t189\nEWR\tMIA\t188\nCLT\tATL\t186\nDFW\tCLT\t185\nATL\tSFO\t185\nDEN\tLGA\t184\nLGA\tDEN\t182\nCLT\tBOS\t181\nEWR\tDEN\t178\nMIA\tBOS\t178\nEWR\tIAH\t177\nDFW\tSEA\t174\nLAX\tDFW\t174\nMIA\tCLT\t174\nORD\tCLT\t173\nATL\tCLT\t170\nSFO\tBOS\t165\nMIA\tSFO\t164\nDEN\tATL\t164\nLGA\tDFW\t162\nEWR\tCLT\t162\nEWR\tDFW\t159\nDEN\tSEA\t155\nCLT\tDFW\t155\nIAH\tLGA\t151\nSEA\tORD\t149\nCLT\tLGA\t147\nCLT\tMIA\t144\nLAX\tEWR\t140\nIAH\tDEN\t139\nIAH\tLAX\t136\nIAH\tATL\t136\nSFO\tATL\t128\nSEA\tDEN\t126\nLAX\tMIA\t126\nDFW\tBOS\t125\nCLT\tSFO\t125\nATL\tIAH\t124\nLGA\tCLT\t124\nMIA\tIAH\t123\nSFO\tIAH\t122\nDEN\tIAH\t120\nBOS\tSFO\t120\nBOS\tCLT\t116\nLAX\tBOS\t114\nBOS\tLAX\t113\nDEN\tBOS\t112\nCLT\tDEN\t106\nBOS\tMIA\t97\nSEA\tDFW\t97\nIAH\tMIA\t94\nCLT\tLAX\t93\nATL\tSEA\t90\nIAH\tBOS\t90\nIAH\tCLT\t86\nCLT\tIAH\t86\nBOS\tDFW\t85\nBOS\tDEN\t81\nLGA\tIAH\t76\nIAH\tSEA\t76\nEWR\tSEA\t72\nSFO\tMIA\t70\nLAX\tSEA\t69\nSEA\tATL\t68\nDEN\tCLT\t67\nLAX\tCLT\t65\nDEN\tMIA\t65\nSEA\tEWR\t65\nCLT\tSEA\t64\nBOS\tIAH\t63\nSFO\tCLT\t59\nLAX\tIAH\t58\nSEA\tLAX\t48\nSEA\tIAH\t47\nMIA\tDEN\t41\nMIA\tSEA\t40\nSEA\tMIA\t25\nBOS\tSEA\t23\nSEA\tBOS\t21\nSEA\tCLT\t9\n"},{"type":"TEXT","data":""}]},"apps":[],"jobName":"paragraph_1552060947933_400525635","id":"20171123-102627_1669385618","dateCreated":"2019-03-08T16:02:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13224"},{"title":"Add origin_dest column  and display the first 20 rows","text":"// Here we add a column to the DataFrame named orig_dest, consisting of the originating and destination airports\nval df2 = df1.withColumn(\"orig_dest\", concat($\"src\",lit(\"_\"), $\"dst\"))\ndf2.show()\n\n","user":"mapr","dateUpdated":"2019-03-08T17:08:17+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+----------+-----+----+-------+---+---+----------+----------+--------+----------+--------+--------------+-----+---------+\n|                  id|    fldate|month|dofW|carrier|src|dst|crsdephour|crsdeptime|depdelay|crsarrtime|arrdelay|crselapsedtime| dist|orig_dest|\n+--------------------+----------+-----+----+-------+---+---+----------+----------+--------+----------+--------+--------------+-----+---------+\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|BOS|         9|       850|     0.0|      1116|     0.0|         146.0|946.0|  ATL_BOS|\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|BOS|        11|      1122|     8.0|      1349|     0.0|         147.0|946.0|  ATL_BOS|\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|BOS|        14|      1356|     9.0|      1623|     0.0|         147.0|946.0|  ATL_BOS|\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|BOS|        16|      1620|     0.0|      1851|     3.0|         151.0|946.0|  ATL_BOS|\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|BOS|        19|      1940|     6.0|      2210|     0.0|         150.0|946.0|  ATL_BOS|\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|BOS|        12|      1248|     0.0|      1513|     0.0|         145.0|946.0|  ATL_BOS|\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|BOS|        22|      2215|     0.0|        39|     0.0|         144.0|946.0|  ATL_BOS|\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|BOS|        15|      1500|    21.0|      1734|    33.0|         154.0|946.0|  ATL_BOS|\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     WN|ATL|BOS|        15|      1500|   198.0|      1725|   208.0|         145.0|946.0|  ATL_BOS|\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     WN|ATL|BOS|        21|      2055|    14.0|      2330|     0.0|         155.0|946.0|  ATL_BOS|\n|ATL_BOS_2018-01-0...|2018-01-01|    1|   1|     WN|ATL|BOS|        10|      1015|   215.0|      1250|   191.0|         155.0|946.0|  ATL_BOS|\n|ATL_CLT_2018-01-0...|2018-01-01|    1|   1|     AA|ATL|CLT|        11|      1114|     0.0|      1238|     0.0|          84.0|226.0|  ATL_CLT|\n|ATL_CLT_2018-01-0...|2018-01-01|    1|   1|     AA|ATL|CLT|         8|       845|     0.0|      1011|     0.0|          86.0|226.0|  ATL_CLT|\n|ATL_CLT_2018-01-0...|2018-01-01|    1|   1|     AA|ATL|CLT|        15|      1548|     0.0|      1710|     0.0|          82.0|226.0|  ATL_CLT|\n|ATL_CLT_2018-01-0...|2018-01-01|    1|   1|     AA|ATL|CLT|         7|       705|     0.0|       821|     0.0|          76.0|226.0|  ATL_CLT|\n|ATL_CLT_2018-01-0...|2018-01-01|    1|   1|     AA|ATL|CLT|        12|      1226|     0.0|      1347|     0.0|          81.0|226.0|  ATL_CLT|\n|ATL_CLT_2018-01-0...|2018-01-01|    1|   1|     AA|ATL|CLT|        22|      2205|     0.0|      2319|     1.0|          74.0|226.0|  ATL_CLT|\n|ATL_CLT_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|CLT|        22|      2210|    11.0|      2324|     0.0|          74.0|226.0|  ATL_CLT|\n|ATL_CLT_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|CLT|        15|      1543|     1.0|      1659|     0.0|          76.0|226.0|  ATL_CLT|\n|ATL_CLT_2018-01-0...|2018-01-01|    1|   1|     DL|ATL|CLT|        10|      1008|     0.0|      1124|     0.0|          76.0|226.0|  ATL_CLT|\n+--------------------+----------+-----+----+-------+---+---+----------+----------+--------+----------+--------+--------------+-----+---------+\nonly showing top 20 rows\n\ndf1: org.apache.spark.sql.DataFrame = [id: string, fldate: string ... 13 more fields]\n"}]},"apps":[],"jobName":"paragraph_1552060947902_1482335676","id":"20171129-221736_959969733","dateCreated":"2019-03-08T16:02:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13205","focus":true},{"text":"%md\nStratified Sampling\nIn order to ensure that our model is sensitive to the delayed samples we can put the two sample types on the same footing using stratified sampling. The DataFrames sampleBy() function does this when provided with fractions of each sample type to be returned. Here, we're keeping all instances of delayed, but downsampling the not delayed instances to 29%, then displaying the results\n","user":"mapr","dateUpdated":"2019-03-08T16:02:27+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Stratified Sampling<br/>In order to ensure that our model is sensitive to the delayed samples we can put the two sample types on the same footing using stratified sampling. The DataFrames sampleBy() function does this when provided with fractions of each sample type to be returned. Here, we&rsquo;re keeping all instances of delayed, but downsampling the not delayed instances to 29%, then displaying the results</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1552060947934_-2054268978","id":"20190305-223154_803200822","dateCreated":"2019-03-08T16:02:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13225"},{"title":"Stratify the sampling to fewer Not Delayed","text":"// keep all delayed , keep 13% not delayed\nval fractions = Map(0.0 -> .13, 1.0->1.0) // \nval df3 =df2.stat.sampleBy(\"delayed\", fractions, 36L)\n// original distribution\ndf2.groupBy(\"delayed\").count.show\n// now\ndf3.groupBy(\"delayed\").count.show\n","user":"mapr","dateUpdated":"2019-03-08T16:02:27+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+------+\n|delayed| count|\n+-------+------+\n|    0.0|253309|\n|    1.0| 29319|\n+-------+------+\n\n+-------+-----+\n|delayed|count|\n+-------+-----+\n|    0.0|33063|\n|    1.0|29319|\n+-------+-----+\n\nfractions: scala.collection.immutable.Map[Double,Double] = Map(0.0 -> 0.13, 1.0 -> 1.0)\ndf3: org.apache.spark.sql.DataFrame = [id: string, fldate: string ... 14 more fields]\n"}]},"apps":[],"jobName":"paragraph_1552060947936_885289571","id":"20171123-050717_1328190278","dateCreated":"2019-03-08T16:02:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13226"},{"title":"Split Into Training and Test set","text":"val splitSeed = 5043\nval Array(trainingData, testData) = df3.randomSplit(Array(0.7, 0.3), splitSeed)\ndf2.unpersist\ndf3.unpersist\ntrainingData.cache\ntrainingData.groupBy(\"delayed\").count.show","user":"mapr","dateUpdated":"2019-03-08T16:02:27+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+-----+\n|delayed|count|\n+-------+-----+\n|    0.0|23149|\n|    1.0|20500|\n+-------+-----+\n\nsplitSeed: Int = 5043\ntrainingData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: string, fldate: string ... 14 more fields]\ntestData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: string, fldate: string ... 14 more fields]\n"}]},"apps":[],"jobName":"paragraph_1552060947937_687597713","id":"20190305-223741_1028691135","dateCreated":"2019-03-08T16:02:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13227"},{"text":"%md\nIn order for the features to be used by a machine learning algorithm, they must be transformed and put into Feature Vectors, which are vectors of numbers \nrepresenting the value for each feature.\n<img src=\"https://mapr.com/blog/fast-data-processing-pipeline-predicting-flight-delays-using-apache-apis-pt-1/assets/reference-learning-spark.png\">\nSpark ML provides a uniform set of high-level APIs built on top of DataFrames. We will use an ML Pipeline to pass the data through transformers in order to extract the features and an estimator to produce the model.\n<img src=\"https://mapr.com/blog/fast-data-processing-pipeline-predicting-flight-delays-using-apache-apis-pt-1/assets/ml-pipeline.png\">\nTransformer: A Transformer is an algorithm which transforms one DataFrame into another DataFrame. We will use a transformer to get a DataFrame with a features vector column.\nEstimator: An Estimator is an algorithm which can be fit on a DataFrame to produce a Transformer. We will use a an estimator to train a model which can transform data to get predictions.\nPipeline: A Pipeline chains multiple Transformers and Estimators together to specify a ML workflow.","user":"mapr","dateUpdated":"2019-03-08T16:02:27+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>In order for the features to be used by a machine learning algorithm, they must be transformed and put into Feature Vectors, which are vectors of numbers<br/>representing the value for each feature.<br/><img src=\"https://mapr.com/blog/fast-data-processing-pipeline-predicting-flight-delays-using-apache-apis-pt-1/assets/reference-learning-spark.png\"><br/>Spark ML provides a uniform set of high-level APIs built on top of DataFrames. We will use an ML Pipeline to pass the data through transformers in order to extract the features and an estimator to produce the model.<br/><img src=\"https://mapr.com/blog/fast-data-processing-pipeline-predicting-flight-delays-using-apache-apis-pt-1/assets/ml-pipeline.png\"><br/>Transformer: A Transformer is an algorithm which transforms one DataFrame into another DataFrame. We will use a transformer to get a DataFrame with a features vector column.<br/>Estimator: An Estimator is an algorithm which can be fit on a DataFrame to produce a Transformer. We will use a an estimator to train a model which can transform data to get predictions.<br/>Pipeline: A Pipeline chains multiple Transformers and Estimators together to specify a ML workflow.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1552060947938_479939068","id":"20190305-223449_516878384","dateCreated":"2019-03-08T16:02:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13228"},{"text":"%md\n## Feature Extraction and Pipelining\nThe ML package needs the label and feature vector to be added as columns to the input dataframe. We set up a pipeline to pass the data through transformers in order to extract the features and label. \nA StringIndexer is used to to encode a string column to a column of number indices.\nA Bucketizer is used to add a label of delayed 0/1. \nA VectorAssembler combines a given list of columns into a single feature vector column.\n","user":"mapr","dateUpdated":"2019-03-08T16:02:27+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Feature Extraction and Pipelining</h2>\n<p>The ML package needs the label and feature vector to be added as columns to the input dataframe. We set up a pipeline to pass the data through transformers in order to extract the features and label.<br/>A StringIndexer is used to to encode a string column to a column of number indices.<br/>A Bucketizer is used to add a label of delayed 0/1.<br/>A VectorAssembler combines a given list of columns into a single feature vector column.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1552060947941_1832237879","id":"20170603-184811_78732818","dateCreated":"2019-03-08T16:02:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13229"},{"title":"Use a StringIndexer  to encode categorical columns","text":"// categorical Column names\nval categoricalColumns = Array(\"carrier\", \"src\", \"dst\", \"dofW\", \"orig_dest\")\n\n// a StringIndexer will encode a string categorial column into a column of numbers\nval stringIndexers = categoricalColumns.map { colName =>\n      new StringIndexer()\n        .setInputCol(colName)\n        .setOutputCol(colName + \"Indexed\")\n        .fit(trainingData)\n}\n","user":"mapr","dateUpdated":"2019-03-08T16:02:27+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"categoricalColumns: Array[String] = Array(carrier, src, dst, dofW, orig_dest)\nstringIndexers: Array[org.apache.spark.ml.feature.StringIndexerModel] = Array(strIdx_d58b3e54de47, strIdx_f7bc9ff03c99, strIdx_a405e0676a8f, strIdx_88eb6185a2fe, strIdx_e516550745f8)\n"}]},"apps":[],"jobName":"paragraph_1552060947942_-313175546","id":"20170508-150543_958647761","dateCreated":"2019-03-08T16:02:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13230"},{"title":"Use VectorAssembler, a transformer,  to put features into a feature vector column","text":"//a Bucketizer is used to add a label column of delayed 0/1.\nval labeler = new Bucketizer().setInputCol(\"depdelay\").setOutputCol(\"label\").setSplits(Array( 0.0, 40.0, Double.PositiveInfinity))\n\n// list of all the feature columns\nval featureCols = Array(\"carrierIndexed\", \"dstIndexed\", \"srcIndexed\", \"dofWIndexed\", \"orig_destIndexed\",\"crsdephour\", \"crsdeptime\", \"crsarrtime\",\"crselapsedtime\", \"dist\")\n\n//The VectorAssembler combines a given list of columns into a single feature vector column. \nval assembler = new VectorAssembler().setInputCols(featureCols).setOutputCol(\"features\")\n\n","user":"mapr","dateUpdated":"2019-03-08T16:02:27+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"labeler: org.apache.spark.ml.feature.Bucketizer = bucketizer_df359ad82d2b\nfeatureCols: Array[String] = Array(carrierIndexed, dstIndexed, srcIndexed, dofWIndexed, orig_destIndexed, crsdephour, crsdeptime, crsarrtime, crselapsedtime, dist)\nassembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_3a197cff8cf2\n"}]},"apps":[],"jobName":"paragraph_1552060947943_-974974989","id":"20170524-223310_2121058884","dateCreated":"2019-03-08T16:02:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13231"},{"title":"Create Random Forest Estimator , set Label and Feature Columns ","text":"// The final element in our ml pipeline is an estimator (a random forest classifier), \n// which will training on the vector of label and features.\nval rf = new RandomForestClassifier().setLabelCol(\"label\").setFeaturesCol(\"features\").setNumTrees(10).setMaxBins(1000).setMaxDepth(8)    ","user":"mapr","dateUpdated":"2019-03-08T16:02:27+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"rf: org.apache.spark.ml.classification.RandomForestClassifier = rfc_13116ad261f9\n"}]},"apps":[],"jobName":"paragraph_1552060947945_-338731910","id":"20170603-185445_276463997","dateCreated":"2019-03-08T16:02:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13232"},{"text":"%md\n## Setup Spark ML pipeline stages\nSet up a pipeline to pass the data through transformers to extract the features and label, and pass this to a random forest estimator to fit the model \n","user":"mapr","dateUpdated":"2019-03-08T16:02:27+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Setup Spark ML pipeline stages</h2>\n<p>Set up a pipeline to pass the data through transformers to extract the features and label, and pass this to a random forest estimator to fit the model</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1552060947946_1333349867","id":"20170601-154525_1033166149","dateCreated":"2019-03-08T16:02:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13233"},{"title":"Set up pipeline with  feature transformers and model estimator","text":"// Below we chain the stringindexers, vector assembler and randomforest in a Pipeline.\nval steps = stringIndexers ++ Array(labeler, assembler, rf)\nval pipeline = new Pipeline().setStages(steps)\n","user":"mapr","dateUpdated":"2019-03-08T16:02:27+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"steps: Array[org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.MLWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.MLWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.MLWritable}}] = Array(strIdx_d58b3e54de47, strIdx_f7bc9ff03c99, strIdx_a405e0676a8f, strIdx_88eb6185a2fe, strIdx_e516550745f8, bucketizer_df359ad82d2b, vecAssembler_3a197cff8cf2, rfc_13116ad261f9)\npipeline: org.apache.spark.ml.Pipeline = pipeline_68b4d25e1c79\n"}]},"apps":[],"jobName":"paragraph_1552060947947_-1422741482","id":"20170508-151557_1422077156","dateCreated":"2019-03-08T16:02:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13234"},{"title":"Train the Model","text":"val model = pipeline.fit(trainingData)","user":"mapr","dateUpdated":"2019-03-08T16:02:27+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"model: org.apache.spark.ml.PipelineModel = pipeline_68b4d25e1c79\n"}]},"apps":[],"jobName":"paragraph_1552060947949_-130360569","id":"20171129-113112_994877662","dateCreated":"2019-03-08T16:02:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13235"},{"title":"Print out the feature importances for the random forest model","text":"// Print out the feature importances\nval rfm = model.stages.last.asInstanceOf[RandomForestClassificationModel]\nval featureImportances =rfm.featureImportances\nassembler.getInputCols.zip(featureImportances.toArray).sortBy(-_._2).foreach { case (feat, imp) => println(s\"feature: $feat, importance: $imp\") }\n\nassembler.getInputCols.zip(featureImportances.toArray).sortBy(-_._2).foreach { case (feat, imp) => println(s\"feature: $feat, importance: $imp\") }\n","user":"mapr","dateUpdated":"2019-03-08T16:02:27+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"feature: crsdeptime, importance: 0.28351763249788353\nfeature: orig_destIndexed, importance: 0.19366046264846604\nfeature: crsarrtime, importance: 0.18539901941274597\nfeature: crsdephour, importance: 0.11189215100129155\nfeature: dofWIndexed, importance: 0.06079935925478266\nfeature: crselapsedtime, importance: 0.05199848773217375\nfeature: srcIndexed, importance: 0.041716384518939956\nfeature: dstIndexed, importance: 0.03598083086117173\nfeature: dist, importance: 0.019475337697743626\nfeature: carrierIndexed, importance: 0.01556033437480119\nfeature: crsdeptime, importance: 0.28351763249788353\nfeature: orig_destIndexed, importance: 0.19366046264846604\nfeature: crsarrtime, importance: 0.18539901941274597\nfeature: crsdephour, importance: 0.11189215100129155\nfeature: dofWIndexed, importance: 0.06079935925478266\nfeature: crselapsedtime, importance: 0.05199848773217375\nfeature: srcIndexed, importance: 0.041716384518939956\nfeature: dstIndexed, importance: 0.03598083086117173\nfeature: dist, importance: 0.019475337697743626\nfeature: carrierIndexed, importance: 0.01556033437480119\nrfm: org.apache.spark.ml.classification.RandomForestClassificationModel = RandomForestClassificationModel (uid=rfc_26f2ad97fdc0) with 10 trees\nfeatureImportances: org.apache.spark.ml.linalg.Vector = (10,[0,1,2,3,4,5,6,7,8,9],[0.01556033437480119,0.03598083086117173,0.041716384518939956,0.06079935925478266,0.19366046264846604,0.11189215100129155,0.28351763249788353,0.18539901941274597,0.05199848773217375,0.019475337697743626])\n"}]},"apps":[],"jobName":"paragraph_1552060947950_-1071414642","id":"20171122-232203_359528816","dateCreated":"2019-03-08T16:02:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13236"},{"text":"%md\r\n## Evaluate the model on a test set\r\nThe actual performance of the model can be determined using the test data set which has not been used for any training. We'll transform the test set with the model pipeline, which will map the features according to the same recipe.  \r\n","user":"mapr","dateUpdated":"2019-03-08T16:02:27+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Evaluate the model on a test set</h2>\n<p>The actual performance of the model can be determined using the test data set which has not been used for any training. We&rsquo;ll transform the test set with the model pipeline, which will map the features according to the same recipe.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1552060947951_-1008260585","id":"20170602-155317_1487132664","dateCreated":"2019-03-08T16:02:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13237"},{"title":"Get Predictions from Test data","text":"//transform the test set with the model pipeline,\n//which will map the features according to the same recipe\nval predictions = model.transform(testData)\n","user":"mapr","dateUpdated":"2019-03-08T16:02:27+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"predictions: org.apache.spark.sql.DataFrame = [id: string, fldate: string ... 24 more fields]\n"}]},"apps":[],"jobName":"paragraph_1552060947953_1981604575","id":"20170508-155848_1997894070","dateCreated":"2019-03-08T16:02:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13238"},{"text":"%md\nAccuracy is measured by the area under the ROC curve. The area measures the ability of the test to correctly classify true positives from false positives. A random predictor would have .5 accuracy. The closer the value is to 1 the better its predictions are. \n\n","user":"mapr","dateUpdated":"2019-03-08T16:02:27+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Accuracy is measured by the area under the ROC curve. The area measures the ability of the test to correctly classify true positives from false positives. A random predictor would have .5 accuracy. The closer the value is to 1 the better its predictions are.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1552060947954_2069631836","id":"20170602-161538_1648758337","dateCreated":"2019-03-08T16:02:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13239"},{"title":"Evaluate the predictions accuracy","text":"val evaluator = new BinaryClassificationEvaluator()  \nval areaUnderROC = evaluator.evaluate(predictions)\nprintln(\"areaUnderROC \"  + areaUnderROC)\n","user":"mapr","dateUpdated":"2019-03-08T16:02:27+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"areaUnderROC 0.6734984453656299\nevaluator: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_0bf52fe9b7aa\nareaUnderROC: Double = 0.6734984453656299\n"}]},"apps":[],"jobName":"paragraph_1552060947955_970639427","id":"20170602-155622_1453197792","dateCreated":"2019-03-08T16:02:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13240"},{"text":"%md\nBelow, we calculate some more metrics. The number of false/true positive and negative predictions is also useful:\n\nTrue positives are how often the model correctly predicted delayed flights.\nFalse positives are how often the model incorrectly predicted delayed flights.\nTrue negatives indicate how often the model correctly predicted not delayed flights.\nFalse negatives indicate how often the model incorrectly predicted not delayed flights.","user":"mapr","dateUpdated":"2019-03-08T16:02:27+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Below, we calculate some more metrics. The number of false/true positive and negative predictions is also useful:</p>\n<p>True positives are how often the model correctly predicted delayed flights.<br/>False positives are how often the model incorrectly predicted delayed flights.<br/>True negatives indicate how often the model correctly predicted not delayed flights.<br/>False negatives indicate how often the model incorrectly predicted not delayed flights.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1552060947956_2126940092","id":"20190305-233843_231265861","dateCreated":"2019-03-08T16:02:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13241"},{"title":"Calculate some more metrics","text":"val lp = predictions.select(  \"prediction\",\"label\")\nval counttotal = predictions.count().toDouble\nval correct = lp.filter($\"label\" === $\"prediction\").count().toDouble\nval wrong = lp.filter(\"label != prediction\").count().toDouble\nval ratioWrong=wrong/counttotal\nval ratioCorrect=correct/counttotal\nval truen =( lp.filter($\"label\" === 0.0).filter($\"label\" === $\"prediction\").count()) /counttotal\nval truep = (lp.filter($\"label\" === 1.0).filter($\"label\" === $\"prediction\").count())/counttotal\nval falsen = (lp.filter($\"label\" === 0.0).filter(not($\"label\" === $\"prediction\")).count())/counttotal\nval falsep = (lp.filter($\"label\" === 1.0).filter(not($\"label\" === $\"prediction\")).count())/counttotal\n","user":"mapr","dateUpdated":"2019-03-08T16:02:27+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"lp: org.apache.spark.sql.DataFrame = [prediction: double, label: double]\ncounttotal: Double = 18733.0\ncorrect: Double = 11799.0\nwrong: Double = 6934.0\nratioWrong: Double = 0.3701489350344312\nratioCorrect: Double = 0.6298510649655688\ntruen: Double = 0.34265734265734266\ntruep: Double = 0.28719372230822615\nfalsen: Double = 0.18486094058613142\nfalsep: Double = 0.1852879944482998\n"}]},"apps":[],"jobName":"paragraph_1552060947958_1003326520","id":"20181015-224231_2027590368","dateCreated":"2019-03-08T16:02:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13242"},{"text":"%md \nHow do you think the accuracy could be improved?\nAdd more data?\nTune the model ?\nAdd more historical data with features such as such as: holidays, bad weather, airport or airline carrier problems.","user":"mapr","dateUpdated":"2019-03-08T16:02:27+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>How do you think the accuracy could be improved?<br/>Add more data?<br/>Tune the model ?<br/>Add more historical data with features such as such as: holidays, bad weather, airport or airline carrier problems.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1552060947959_-1269548202","id":"20181004-211413_1357524146","dateCreated":"2019-03-08T16:02:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13243"},{"title":"Save the model to use with streaming data","text":"var dir =\"/user/mapr/model/\"\nmodel.write.overwrite().save(dir)\ntrainingData.unpersist","user":"mapr","dateUpdated":"2019-03-08T16:02:27+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"dir: String = /user/mapr/model/\nres8: trainingData.type = [id: string, fldate: string ... 14 more fields]\n"}]},"apps":[],"jobName":"paragraph_1552060947960_-1585723425","id":"20181015-213933_2083506099","dateCreated":"2019-03-08T16:02:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13244"},{"user":"mapr","dateUpdated":"2019-03-08T16:02:27+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1552060947962_-816139212","id":"20171122-091021_1615582434","dateCreated":"2019-03-08T16:02:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13245"}],"name":"FlightDelayPrediction","id":"2E689NKHY","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}